{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行环境： python-3.6.12 tensorflow-1.14.0  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from geopy.distance import geodesic\n",
    "import scipy.stats\n",
    "import math\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler #正态分布归一化\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIP_E = np.load('SIP_Time5_Weather10-Interval30-201701_03.npy')\n",
    "SIP_30min_flow = np.load('SIP_30m_Flow.npy')\n",
    "\n",
    "\n",
    "#地区数量Node\n",
    "location_degree = SIP_30min_flow.shape[0]\n",
    "\n",
    "#设置一个batch_size大小以及总共有多少块\n",
    "batch_size = 1\n",
    "batch_number = np.int(4320/18/batch_size)\n",
    "one_epoch_size = np.int((4320-17)/batch_size)\n",
    "#M\n",
    "w3_degree = 4\n",
    "#\n",
    "M5_degree = 8\n",
    "#GNN\n",
    "w6_degree = 64\n",
    "\n",
    "#将12个按多少分一组\n",
    "GNN_degree = 3\n",
    "number_of_GNN = np.int(12/3)\n",
    "#LSTM_output_seq\n",
    "output_seq_len = 6\n",
    "#\n",
    "num_stacked_layers = 1\n",
    "hidden_dim_1 = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIP_30min_flow_mean = np.mean(SIP_30min_flow,axis=1)\n",
    "var = np.std(SIP_30min_flow,axis=1)\n",
    "var.shape\n",
    "SIP_30min_flow_mean = np.expand_dims(SIP_30min_flow_mean,1)\n",
    "var = np.expand_dims(var,1)\n",
    "SIP_30min_flow_mean = np.tile(SIP_30min_flow_mean,(108,4320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((108, 4320), (108, 1))"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SIP_30min_flow_mean = np.mean(SIP_30min_flow,axis=1)\n",
    "SIP_30min_flow_mean = np.expand_dims(SIP_30min_flow_mean,1)\n",
    "\n",
    "SIP_30min_flow_mean = np.tile(SIP_30min_flow_mean,(1,4320))\n",
    "#var = np.tile(var,(1,4320))\n",
    "SIP_30min_flow_mean.shape,var.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/SIP_30m_mean.npy',SIP_30min_flow_mean)\n",
    "np.save('data/SIP_30m_var.npy',var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization \n",
    "SIP_30min_flow_norm = (SIP_30min_flow - SIP_30min_flow_mean)/var\n",
    "# reverse\n",
    "SIP_30min_flow_rev = SIP_30min_flow_norm * var + SIP_30min_flow_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取外部变量x-y个时间段的平均值\n",
    "def extract_x_sample_e(matrix, x, y):\n",
    "    return np.mean(matrix[x:y, :], axis=0)\n",
    "\n",
    "\n",
    "# 返回从x处截断的两个三维矩阵\n",
    "def cut_matrix(matrix, x):\n",
    "    return matrix[:,:,0:x], matrix[:,:,x:]\n",
    "\n",
    "\n",
    "# 取矩阵\n",
    "def get_target_matrix(matrix, x, y):\n",
    "    return matrix[:, x:y]\n",
    "\n",
    "\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#取batch组数据 返回训练数据batch*1*15  观测数据batch*108*6  每18个interval一个sample\n",
    "def get_batch_data(batch_size,number_of_batch):\n",
    "    batch_input1_data = np.zeros((batch_size,1,15))\n",
    "    batch_input2_data = np.zeros((batch_size,108,12))\n",
    "    batch_real_data = np.zeros((batch_size,108,6))\n",
    "  #  tt = np.random.choice(range(14*48,4320), batch_size)\n",
    "    \n",
    "    #normalization_data = np.zeros((108,18))\n",
    "    for i in range(batch_size):\n",
    "        #j = np.random.randint(0,240)\n",
    "        \n",
    "        #batch_input_data[i] = np.expand_dims(extract_x_sample_e(SIP_E,number_of_batch*batch_size+i ,number_of_batch*batch_size+i+12 ),axis=0)\n",
    "        #batch_real_data[i] = get_target_matrix(SIP_30min_flow, number_of_batch*batch_size+i+12 ,number_of_batch*batch_size+i+18)\n",
    "        \n",
    "        batch_input1_data[i] = np.expand_dims(extract_x_sample_e(SIP_E,number_of_batch*batch_size*18+i*18 ,number_of_batch*batch_size*18+i*18+12 ),axis=0)\n",
    "        batch_input2_data[i] = get_target_matrix(SIP_30min_flow,number_of_batch*batch_size*18+i*18,number_of_batch*batch_size*18+i*18+12)\n",
    "        # batch_real_data[i] = get_target_matrix(SIP_30min_flow, number_of_batch*batch_size*18+i*18+12 ,number_of_batch*batch_size*18+i*18+18)\n",
    "        batch_real_data[i] = get_target_matrix(SIP_30min_flow, number_of_batch*batch_size*18+i*18+12 ,number_of_batch*batch_size*18+i*18+18)\n",
    "        \n",
    "        #batch_input_data[i] = np.expand_dims(extract_x_sample_e(SIP_E,j*18 ,j*18+12 ),axis=0)\n",
    "        #batch_real_data[i] = get_target_matrix(SIP_30min_flow, j*18+12 ,j*18+18)\n",
    "        \n",
    "        \n",
    "    return batch_input1_data,batch_input2_data,batch_real_data.transpose(0,2,1)\n",
    "a,b,c = get_batch_data(1,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_matrix = np.eye(location_degree,dtype=np.float32)\n",
    "location_matrix = np.expand_dims(location_matrix,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(feed_previous = False,reuse_variables=False):\n",
    "    tf.reset_default_graph()\n",
    "    global_step = tf.Variable(\n",
    "                  initial_value=0,\n",
    "                  name=\"global_step\",\n",
    "                  trainable=False,\n",
    "                  collections=[tf.GraphKeys.GLOBAL_STEP, tf.GraphKeys.GLOBAL_VARIABLES])\n",
    "\n",
    "    def get_shape_sim_tf(x,y):\n",
    "        x_diff = x - tf.roll(x, shift=1, axis=0)\n",
    "        y_diff = y - tf.roll(y, shift=1, axis=0)\n",
    "        x_ycos = (-tf.reduce_sum(tf.multiply(x,y))/(tf.sqrt(tf.reduce_sum(tf.multiply(x,x))+0.01)*tf.sqrt(tf.reduce_sum(tf.multiply(y,y))))+0.01)\n",
    "        #x_ydist = tf.sqrt(tf.reduce_sum(tf.pow(x_diff-y_diff, 2)))\n",
    "\n",
    "        return x_ycos\n",
    "\n",
    "    #\n",
    "    x1 = tf.placeholder(tf.float32,[batch_size,1,15])#时间天气向量 batch_size*1*15\n",
    "    x2 = tf.placeholder(tf.float32,[batch_size,108,12])#流量\n",
    "    #ys = tf.placeholder(tf.float32,[batch_size,location_degree,6])#batch_size*108*6\n",
    "    Output_Seq = tf.placeholder(shape=(1, output_seq_len, location_degree),dtype=tf.float32)\n",
    "    \n",
    "    #FC w,b\n",
    "  #  w0 = tf.Variable(tf.random_normal([location_degree,location_degree,M5_degree]),dtype=tf.float32,name='w0')\n",
    "    #  w0 = tf.Variable(tf.random_normal([location_degree,location_degree,M5_degree]),dtype=tf.float32,name='w0')\n",
    "    w0 = tf.Variable(tf.random_normal([location_degree,M5_degree]),dtype=tf.float32,name='w0')\n",
    "    \n",
    "    w1 = tf.Variable(tf.random_normal([M5_degree,1]),dtype=tf.float32,name='w1') # √\n",
    "\n",
    "\n",
    "    #b1 = tf.Variable(tf.truncated_normal([location_degree,location_degree],stddev=0.1),dtype=tf.float32,name='b1')\n",
    "\n",
    "    b2 = tf.Variable(tf.truncated_normal([1,5],stddev=0.1),dtype=tf.float32,name='b2')\n",
    "\n",
    "    w2 = tf.Variable(tf.truncated_normal([5,1]),dtype=tf.float32,name='w2')#5*1    # √\n",
    "\n",
    "    b3 = tf.Variable(tf.truncated_normal([1,10],stddev=0.1),dtype=tf.float32,name='b3')\n",
    "\n",
    "    w3 = tf.Variable(tf.truncated_normal([10,w3_degree]),dtype=tf.float32,name='w3')#10*M\n",
    "\n",
    "    w4 = tf.Variable(tf.truncated_normal([w3_degree,w3_degree]),dtype=tf.float32,name='w4')#M*6\n",
    " \n",
    "    \n",
    "    w4_1 = tf.Variable(tf.truncated_normal([w3_degree,output_seq_len]),dtype=tf.float32,name='w4_1')#M*6\n",
    "    w4_2 = tf.Variable(tf.truncated_normal([output_seq_len,output_seq_len]),dtype=tf.float32,name='w4_2')#M*6\n",
    "    f6_out = tf.Variable(tf.random_normal([batch_size,location_degree,w3_degree]),dtype=tf.float32,name='context')\n",
    "\n",
    "    location_w_matrix = tf.Variable(tf.truncated_normal([location_degree,1,M5_degree]),dtype=tf.float32,name='M5')\n",
    "\n",
    "    def add_layer(inputs,activation_function=None):\n",
    "        #获得时间天气\n",
    "        inputs = tf.cast(inputs,tf.float32)\n",
    "        time_vector ,weather_vector = cut_matrix(inputs, 5) #b*1*5 b*1*10\n",
    "\n",
    "        #(L+b1)*w0*w1\n",
    "#         for i in range(location_degree):\n",
    "#             tf.assign(location_w_matrix[i],tf.matmul(location_matrix[i],w0[i]))\n",
    "      \n",
    "        f1_out = tf.matmul(location_matrix, w0)\n",
    "        f1_out = tf.squeeze(location_w_matrix,axis=1)#108*M\n",
    "        f2_out = tf.matmul(f1_out, w1)\n",
    "\n",
    "        f3_out = tf.map_fn(lambda x: tf.matmul(f2_out, x), time_vector) #b*108*5\n",
    "\n",
    "        f4_out = tf.map_fn(lambda x: tf.matmul(x,w2), f3_out)#b*108*1\n",
    "\n",
    "        f5_out = tf.matmul(f4_out,weather_vector)#b*108*10\n",
    "\n",
    "        wb_mean, wb_var = tf.nn.moments(f5_out, [0,1,2])\n",
    "        scale = tf.Variable(tf.ones([1]))\n",
    "    \n",
    "        offset = tf.Variable(tf.zeros([1]))\n",
    "    \n",
    "        variance_epsilon = 0.001\n",
    "        f5_out = tf.nn.batch_normalization(f5_out, wb_mean, wb_var, offset, scale, variance_epsilon)\n",
    "        \n",
    "        f6_out = tf.map_fn(lambda x: tf.matmul(x,w3), f5_out)#b*108*M\n",
    "        \n",
    "        f6_out = tf.map_fn(lambda x: tf.matmul(x,w4), f6_out)#b*108*M\n",
    "                \n",
    "        Pred_Context_out = tf.map_fn(lambda x: tf.matmul(x,w4_1), f6_out) #b*108*6\n",
    "        \n",
    "        Pred_Context_out = tf.map_fn(lambda x: tf.matmul(x,w4_2), Pred_Context_out) #b*108*6\n",
    "        \n",
    "        if activation_function is None:\n",
    "            outputs = f6_out\n",
    "        else:\n",
    "            outputs = activation_function(f6_out)\n",
    "        return outputs, Pred_Context_out, f6_out\n",
    "    \n",
    "    FC_layer_1, Pred_Context_out,f6_out = add_layer(x1,activation_function=tf.nn.relu)\n",
    "    \n",
    "    Pred_Context_out = tf.transpose(Pred_Context_out,[0,2,1])\n",
    "    print(\"Pred_contextshape: \",Pred_Context_out.shape)\n",
    "    \n",
    "\n",
    "    \n",
    "    #FC——得到b*108*108的参数\n",
    "    w5 = tf.Variable(tf.truncated_normal([w3_degree,location_degree]),dtype=tf.float32,name='w5')\n",
    "    b5 = tf.Variable(tf.truncated_normal([location_degree,w3_degree]),dtype=tf.float32,name='b5')\n",
    "    #Gnn参数\n",
    "    #108*3的b\n",
    "    b6 = tf.Variable(tf.truncated_normal([batch_size,location_degree,GNN_degree]),dtype=tf.float32,name='b6')\n",
    "    w6 = tf.Variable(tf.truncated_normal([GNN_degree, 64]),dtype=tf.float32,name='w6')\n",
    "    w7 = tf.Variable(tf.truncated_normal([64,8]),dtype=tf.float32,name='w7')\n",
    "    w8 = tf.Variable(tf.truncated_normal([8,1]),dtype=tf.float32,name='w8')\n",
    "    \n",
    "   # w9 = tf.Variable(tf.truncated_normal([3,1]),dtype=tf.float32,name='w9')\n",
    "  #  w10 = tf.Variable(tf.truncated_normal([3,1]),dtype=tf.float32,name='w10')\n",
    "  #  w11 = tf.Variable(tf.truncated_normal([3,1]),dtype=tf.float32,name='w11')\n",
    "    \n",
    "    #得到batch个108*108个context矩阵\n",
    "    def get_ADJ(inputs,activation_function=None):\n",
    "        inputs = tf.cast(inputs,tf.float32)\n",
    "        f1 = tf.matmul(inputs,w5)\n",
    "\n",
    "        if activation_function is None:\n",
    "            outputs = f1\n",
    "        else:\n",
    "            outputs = activation_function(f1)\n",
    "        return outputs\n",
    "    \n",
    "    #得到context\n",
    "    ADJ_matrix = get_ADJ(FC_layer_1,activation_function=tf.nn.leaky_relu) \n",
    "    \n",
    "    \n",
    "    def Long_termGCN(input1,  Adj_M):\n",
    "    #输入为b*108*3  b*108*108\n",
    "        #第一层\n",
    "        #得到b*108*3\n",
    "        layer1_temp = tf.matmul(Adj_M, input1)\n",
    "        #print(layer1_temp.shape)\n",
    "        #b*108*3   *    3*M   =  b*108*M\n",
    "        layer_1_output = tf.map_fn(lambda x: tf.matmul(x, w6), layer1_temp)\n",
    "        #b*108*M   *    M*1   =  b*108*1\n",
    "        #layer_1_output = tf.map_fn(lambda x: tf.matmul(x, w7), layer_1_output)\n",
    "        \n",
    "        wb_mean, wb_var = tf.nn.moments(layer_1_output, [0,1,2])\n",
    "        scale = tf.Variable(tf.ones([1]))\n",
    "    \n",
    "        offset = tf.Variable(tf.zeros([1])) \n",
    "    \n",
    "        variance_epsilon = 0.001\n",
    "        layer_1_output = tf.nn.batch_normalization(layer_1_output, wb_mean, wb_var, offset, scale, variance_epsilon)\n",
    "        \n",
    "       \n",
    "        \n",
    "        layer_1_output = tf.map_fn(lambda x: tf.matmul(x, w7), layer_1_output)\n",
    "     #   layer_1_output = tf.matmul(Adj_M, layer_1_output)\n",
    "        \n",
    "        layer_1_output = tf.map_fn(lambda x: tf.matmul(x, w8), layer_1_output)\n",
    "        \n",
    "     #   layer_1_output = tf.map_fn(lambda x: tf.matmul(x, w9), layer_1_output)\n",
    "    #    layer_1_output = tf.map_fn(lambda x: tf.matmul(x, w10), layer_1_output)\n",
    "    #    layer_1_output = tf.map_fn(lambda x: tf.matmul(x, w11), layer_1_output)\n",
    "        return layer_1_output\n",
    "    \n",
    "    \n",
    "    def lstm(Output6,Output_Seq,num_stacked_layers,input_size, hidden_dim,feed_previous): \n",
    "        \n",
    "        weightslstm={\n",
    "         'in':tf.Variable(tf.truncated_normal(shape=[input_size,hidden_dim], stddev=0.1)), \n",
    "         'out':tf.Variable(tf.truncated_normal(shape=[hidden_dim,input_size], stddev=0.1)) \n",
    "        }\n",
    "        biaseslstm={\n",
    "                'in':tf.Variable(tf.truncated_normal(shape=[hidden_dim,], stddev=0.1)), \n",
    "                'out':tf.Variable(tf.truncated_normal(shape=[input_size,], stddev=0.1)) \n",
    "               }\n",
    "\n",
    "        \n",
    "        ## Seq2Seq Parameters\n",
    "        ## Parameters\n",
    "        enc_inp = Output6\n",
    "\n",
    "        print(\"len_enc_inp:\",len(enc_inp))\n",
    "        print(enc_inp)\n",
    "        # Decoder: target outputs\n",
    "        target_seq = [\n",
    "            tf.placeholder(tf.float32, shape=(1, input_size), name=\"y\".format(t))\n",
    "              for t in range(output_seq_len)\n",
    "        ]\n",
    "\n",
    "        target_seq = Output_Seq\n",
    "        print(\"len_target_seq:\",len(target_seq))\n",
    "        dec_inp = [ tf.zeros_like(target_seq[0], dtype=tf.float32, name=\"GO\") ] + target_seq[:-1]\n",
    "    \n",
    "    \n",
    "        cells = []\n",
    "        for i in range(num_stacked_layers):\n",
    "            with tf.variable_scope('RNN_{}'.format(i)):\n",
    "                cells.append(tf.contrib.rnn.LSTMCell(num_units = hidden_dim, activation = tf.nn.relu)) # activation = tf.nn.leaky_relu\n",
    "\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "        def _rnn_decoder(decoder_inputs,\n",
    "                        initial_state,\n",
    "                        cell,\n",
    "                        loop_function=None,\n",
    "                        scope=None):\n",
    "            \n",
    "            state = initial_state\n",
    "            outputs = []\n",
    "            prev = None\n",
    "            for i, inp in enumerate(decoder_inputs):\n",
    "                if loop_function is not None and prev is not None:\n",
    "                    with tf.variable_scope(\"loop_function\", reuse=tf.AUTO_REUSE):\n",
    "                        inp = loop_function(prev, i)\n",
    "                if i > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                output, state = cell(inp, state)\n",
    "                outputs.append(output)\n",
    "                if loop_function is not None:\n",
    "                    prev = output\n",
    "            return outputs, state\n",
    "\n",
    "\n",
    "        def _basic_rnn_seq2seq(encoder_inputs,\n",
    "                              decoder_inputs,\n",
    "                              cell,\n",
    "                              feed_previous,\n",
    "                              dtype=tf.float32,\n",
    "                              scope=None):\n",
    "        #  \"\"\"Basic RNN sequence-to-sequence model.\n",
    "        #  \"\"\"\n",
    "        # Encoder  \n",
    "        # 运行 encoder,把待编码的输入RNN中得到enc_state\n",
    "            enc_cell = cell\n",
    "            _, enc_state = tf.contrib.rnn.static_rnn(enc_cell, encoder_inputs, dtype=dtype)\n",
    "            # Decoder \n",
    "            #如何feed_previous = True 的话 就把先前decoder 输出的放到下一个时间步去进行decoder\n",
    "            if feed_previous:\n",
    "                return _rnn_decoder(decoder_inputs, enc_state, cell, _loop_function)\n",
    "            else:\n",
    "                return _rnn_decoder(decoder_inputs, enc_state, cell)\n",
    "        #print(tf.get_variable_scope().reuse)\n",
    "        def _loop_function(prev, _):\n",
    "          '''Naive implementation of loop function for _rnn_decoder. Transform prev from \n",
    "          dimension [batch_size x hidden_dim] to [batch_size x output_dim], which will be\n",
    "          used as decoder input of next time step '''\n",
    "          return tf.matmul(prev, weightslstm['out']) + biaseslstm['out']\n",
    "        \n",
    "        dec_outputs, dec_memory = _basic_rnn_seq2seq(\n",
    "            enc_inp, \n",
    "            dec_inp,  ##输入带有context的数据\n",
    "            cell, \n",
    "            feed_previous = feed_previous\n",
    "        )\n",
    "        # list:内部是6个1*108tensor张量\n",
    "        Pred = [tf.nn.relu(tf.matmul(i, weightslstm['out'])+biaseslstm['out']) for i in dec_outputs]\n",
    "        return Pred\n",
    "    \n",
    "    Input_Seq_F = []\n",
    "    #number_of_GNN = 4\n",
    "    for i in range(number_of_GNN):\n",
    "        #print(Long_termGCN(x2[:,:,i*3:i*3+3], ADJ_matrix).shape)\n",
    "        output = Long_termGCN(x2[:,:,i*3:i*3+3], ADJ_matrix)\n",
    "        output = tf.squeeze(output,axis=2)\n",
    "        Input_Seq_F.append(output)  \n",
    "    #print(Input_Seq_F)\n",
    "    #4 * 108 * 1\n",
    "    Output_Seq_F = []  # 6*108\n",
    "    for i in range(6):\n",
    "        Output_Seq_F.append(Output_Seq[:,i ,:])\n",
    "    #print(\"output##############################\",Output_Seq_F)\n",
    "\n",
    "    #LSTM\n",
    "    with tf.variable_scope(\"Pred_RNN\"):  \n",
    "            Pred_Result = lstm(Input_Seq_F,Output_Seq_F,num_stacked_layers,108,hidden_dim_1,False)\n",
    "    loss1 = 0\n",
    "    loss2 = 0\n",
    "    loss_shape = 0\n",
    "    for _y, _Y in zip(Output_Seq_F, Pred_Result):\n",
    "        print(_y.shape, _Y.shape)\n",
    "        loss1 += tf.reduce_mean(tf.keras.losses.mse(_y ,_Y))\n",
    "        loss2 += tf.reduce_mean(tf.keras.losses.mape(_y ,_Y))\n",
    "    \n",
    "    Pred_Result = tf.reshape(Pred_Result,[1,output_seq_len, location_degree])\n",
    "    Output_Seq1 = tf.reshape(Output_Seq,[1,output_seq_len, location_degree])\n",
    "    print(\"Pred_rsut:\",Pred_Result.shape)\n",
    "    for i in range(location_degree):\n",
    "       # loss_shape += get_shape_sim_tf(Pred_Context_out[0,:,i],Output_Seq1[0,:,i]) +  get_shape_sim_tf(Pred_Result[0,:,i],Output_Seq1[0,:,i])\n",
    "       loss_shape += get_shape_sim_tf(Pred_Result[0,:,i],Output_Seq1[0,:,i])\n",
    "       #loss_shape += get_shape_sim_tf(Pred_Context_out[0,:,i],Output_Seq1[0,:,i]) \n",
    "    #print(\"#########\",loss1)\n",
    "    loss_FC = tf.reduce_mean(tf.keras.losses.mape(Output_Seq,Pred_Context_out))\n",
    "    print(\"pred_shape:\", Pred_Context_out.shape)\n",
    "   \n",
    "    loss2 = loss2/6\n",
    "    loss_FC = loss_FC/2\n",
    "   \n",
    "    loss_sum =loss2  +  loss_FC  + 0.3 * loss_shape\n",
    "    #print(\"#########\",loss1)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(0.0001,global_step,15,decay_rate=0.98,staircase=True)  \n",
    "    train = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_sum)#学习率下降\n",
    "    saver = tf.train.Saver\n",
    "    context_embed = f6_out \n",
    "    return dict(\n",
    "        x1 = x1,\n",
    "        x2 = x2,\n",
    "        Pred_Context_out = Pred_Context_out,\n",
    "        Pred_Result = Pred_Result, # √  Prediction\n",
    "        Output_Seq = Output_Seq, # √ Ground truth\n",
    "        context_embed = context_embed, # √ Context\n",
    "        loss_FC = loss_FC,\n",
    "        loss1 = loss1,\n",
    "        loss2 = loss2,   # √  \n",
    "        loss_shape = loss_shape, \n",
    "        train = train,      \n",
    "        saver = saver,\n",
    "        ADJ_matrix = ADJ_matrix # √\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred_contextshape:  (1, 6, 108)\n",
      "len_enc_inp: 4\n",
      "[<tf.Tensor 'Squeeze_1:0' shape=(1, 108) dtype=float32>, <tf.Tensor 'Squeeze_2:0' shape=(1, 108) dtype=float32>, <tf.Tensor 'Squeeze_3:0' shape=(1, 108) dtype=float32>, <tf.Tensor 'Squeeze_4:0' shape=(1, 108) dtype=float32>]\n",
      "len_target_seq: 6\n",
      "(1, 108) (1, 108)\n",
      "(1, 108) (1, 108)\n",
      "(1, 108) (1, 108)\n",
      "(1, 108) (1, 108)\n",
      "(1, 108) (1, 108)\n",
      "(1, 108) (1, 108)\n",
      "Pred_rsut: (1, 6, 108)\n",
      "pred_shape: (1, 6, 108)\n"
     ]
    }
   ],
   "source": [
    "graph = build_graph(feed_previous = False,reuse_variables=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0   MAPE 90.30061\n",
      "sum_mpape:  79.67108960151673\n",
      "Epoch: 1   MAPE 76.93974\n",
      "sum_mpape:  64.905561876297\n",
      "Epoch: 2   MAPE 75.5257\n",
      "sum_mpape:  63.579222679138184\n",
      "Epoch: 3   MAPE 74.11926\n",
      "sum_mpape:  63.24172336260478\n",
      "Epoch: 4   MAPE 74.16232\n",
      "sum_mpape:  63.08619360923767\n",
      "Epoch: 5   MAPE 73.39907\n",
      "sum_mpape:  62.66040563583374\n",
      "Epoch: 6   MAPE 73.08414\n",
      "sum_mpape:  62.61097442309062\n",
      "Epoch: 7   MAPE 72.68962\n",
      "sum_mpape:  62.300768502553304\n",
      "Epoch: 8   MAPE 72.337746\n",
      "sum_mpape:  61.79799281756083\n",
      "Epoch: 9   MAPE 71.684586\n",
      "sum_mpape:  61.453480911254886\n",
      "Epoch: 10   MAPE 70.99833\n",
      "sum_mpape:  60.90745412508647\n",
      "Epoch: 11   MAPE 70.32652\n",
      "sum_mpape:  60.6958615620931\n",
      "Epoch: 12   MAPE 69.61961\n",
      "sum_mpape:  60.309106254577635\n",
      "Epoch: 13   MAPE 68.92993\n",
      "sum_mpape:  60.08208301862081\n",
      "Epoch: 14   MAPE 67.99284\n",
      "sum_mpape:  59.61430368423462\n",
      "Epoch: 15   MAPE 67.19273\n",
      "sum_mpape:  59.15302845637004\n",
      "Epoch: 16   MAPE 66.21699\n",
      "sum_mpape:  58.57808105150858\n",
      "Epoch: 17   MAPE 65.54939\n",
      "sum_mpape:  58.10820333162943\n",
      "Epoch: 18   MAPE 64.56783\n",
      "sum_mpape:  57.205474583307904\n",
      "Epoch: 19   MAPE 63.425934\n",
      "sum_mpape:  56.44894658724467\n",
      "Epoch: 20   MAPE 61.981777\n",
      "sum_mpape:  55.64173919359843\n",
      "Epoch: 21   MAPE 60.26503\n",
      "sum_mpape:  54.85910733540853\n",
      "Epoch: 22   MAPE 57.85589\n",
      "sum_mpape:  53.58900866508484\n",
      "Epoch: 23   MAPE 57.044365\n",
      "sum_mpape:  53.18448203404744\n",
      "Epoch: 24   MAPE 57.645325\n",
      "sum_mpape:  52.62394649187724\n",
      "Epoch: 25   MAPE 56.360382\n",
      "sum_mpape:  52.23175441424052\n",
      "Epoch: 26   MAPE 55.723984\n",
      "sum_mpape:  51.90264228185018\n",
      "Epoch: 27   MAPE 55.11328\n",
      "sum_mpape:  51.57404168446859\n",
      "Epoch: 28   MAPE 54.482662\n",
      "sum_mpape:  51.283207416534424\n",
      "Epoch: 29   MAPE 54.287987\n",
      "sum_mpape:  51.069660329818724\n",
      "Epoch: 30   MAPE 53.8016\n",
      "sum_mpape:  50.84688278834025\n",
      "Epoch: 31   MAPE 53.554626\n",
      "sum_mpape:  50.63259647687276\n",
      "Epoch: 32   MAPE 53.282616\n",
      "sum_mpape:  50.415427668889365\n",
      "Epoch: 33   MAPE 53.006756\n",
      "sum_mpape:  50.207339970270795\n",
      "Epoch: 34   MAPE 52.92584\n",
      "sum_mpape:  49.942529296875\n",
      "Epoch: 35   MAPE 52.674255\n",
      "sum_mpape:  49.6115173180898\n",
      "Epoch: 36   MAPE 51.97999\n",
      "sum_mpape:  49.37828118006389\n",
      "Epoch: 37   MAPE 52.027412\n",
      "sum_mpape:  49.23707049687703\n",
      "Epoch: 38   MAPE 52.04635\n",
      "sum_mpape:  49.14763479232788\n",
      "Epoch: 39   MAPE 51.629513\n",
      "sum_mpape:  48.967177136739096\n",
      "Epoch: 40   MAPE 51.635143\n",
      "sum_mpape:  48.60651335716248\n",
      "Epoch: 41   MAPE 51.17826\n",
      "sum_mpape:  48.3538241704305\n",
      "Epoch: 42   MAPE 50.919537\n",
      "sum_mpape:  48.23094620704651\n",
      "Epoch: 43   MAPE 50.85194\n",
      "sum_mpape:  48.102888854344684\n",
      "Epoch: 44   MAPE 50.742474\n",
      "sum_mpape:  48.02024935086568\n",
      "Epoch: 45   MAPE 50.810543\n",
      "sum_mpape:  47.94828467369079\n",
      "Epoch: 46   MAPE 50.54153\n",
      "sum_mpape:  47.88031126658122\n",
      "Epoch: 47   MAPE 50.71063\n",
      "sum_mpape:  47.798532994588214\n",
      "Epoch: 48   MAPE 50.49867\n",
      "sum_mpape:  47.89592374165853\n",
      "Epoch: 49   MAPE 50.72836\n",
      "sum_mpape:  47.66783881187439\n",
      "Epoch: 50   MAPE 50.58212\n",
      "sum_mpape:  47.585090510050456\n",
      "Epoch: 51   MAPE 50.51194\n",
      "sum_mpape:  47.553964646657306\n",
      "Epoch: 52   MAPE 50.32962\n",
      "sum_mpape:  47.28598655064901\n",
      "Epoch: 53   MAPE 50.312958\n",
      "sum_mpape:  47.02046758333842\n",
      "Epoch: 54   MAPE 50.519386\n",
      "sum_mpape:  46.541340843836466\n",
      "Epoch: 55   MAPE 49.74436\n",
      "sum_mpape:  46.40063562393188\n",
      "Epoch: 56   MAPE 49.771866\n",
      "sum_mpape:  46.253995498021446\n",
      "Epoch: 57   MAPE 49.610893\n",
      "sum_mpape:  46.25456252098083\n",
      "Epoch: 58   MAPE 50.272003\n",
      "sum_mpape:  46.17410701115926\n",
      "Epoch: 59   MAPE 50.05044\n",
      "sum_mpape:  46.11852340698242\n",
      "Epoch: 60   MAPE 49.775574\n",
      "sum_mpape:  46.04387345314026\n",
      "Epoch: 61   MAPE 49.736084\n",
      "sum_mpape:  45.93587910334269\n",
      "Epoch: 62   MAPE 49.550415\n",
      "sum_mpape:  45.84962854385376\n",
      "Epoch: 63   MAPE 49.556824\n",
      "sum_mpape:  45.796064631144205\n",
      "Epoch: 64   MAPE 49.313976\n",
      "sum_mpape:  45.70717636744181\n",
      "Epoch: 65   MAPE 49.428143\n",
      "sum_mpape:  45.68062057495117\n",
      "Epoch: 66   MAPE 49.199947\n",
      "sum_mpape:  45.57747586568197\n",
      "Epoch: 67   MAPE 49.489098\n",
      "sum_mpape:  45.54283068974813\n",
      "Epoch: 68   MAPE 49.359142\n",
      "sum_mpape:  45.48583687146505\n",
      "Epoch: 69   MAPE 49.32872\n",
      "sum_mpape:  45.44986964861552\n",
      "Epoch: 70   MAPE 49.374905\n",
      "sum_mpape:  45.41871951421102\n",
      "Epoch: 71   MAPE 49.14701\n",
      "sum_mpape:  45.35439780553182\n",
      "Epoch: 72   MAPE 49.19455\n",
      "sum_mpape:  45.21365270614624\n",
      "Epoch: 73   MAPE 49.18432\n",
      "sum_mpape:  45.10350743929545\n",
      "Epoch: 74   MAPE 48.98613\n",
      "sum_mpape:  45.0249755859375\n",
      "Epoch: 75   MAPE 48.943554\n",
      "sum_mpape:  44.88415233294169\n",
      "Epoch: 76   MAPE 48.889736\n",
      "sum_mpape:  44.6731048266093\n",
      "Epoch: 77   MAPE 48.60499\n",
      "sum_mpape:  44.50674522717794\n",
      "Epoch: 78   MAPE 48.438995\n",
      "sum_mpape:  44.414400815963745\n",
      "Epoch: 79   MAPE 48.470516\n",
      "sum_mpape:  44.34931155840556\n",
      "Epoch: 80   MAPE 48.386345\n",
      "sum_mpape:  44.30675121943156\n",
      "Epoch: 81   MAPE 48.497353\n",
      "sum_mpape:  44.256707429885864\n",
      "Epoch: 82   MAPE 48.412354\n",
      "sum_mpape:  44.19369963010152\n",
      "Epoch: 83   MAPE 48.52053\n",
      "sum_mpape:  44.06866709391276\n",
      "Epoch: 84   MAPE 48.53167\n",
      "sum_mpape:  43.757261768976846\n",
      "Epoch: 85   MAPE 48.14387\n",
      "sum_mpape:  43.460046005249026\n",
      "Epoch: 86   MAPE 47.812893\n",
      "sum_mpape:  43.29116180737813\n",
      "Epoch: 87   MAPE 47.446304\n",
      "sum_mpape:  43.183991543451945\n",
      "Epoch: 88   MAPE 47.387783\n",
      "sum_mpape:  43.09259085655212\n",
      "Epoch: 89   MAPE 47.457115\n",
      "sum_mpape:  43.02270224889119\n",
      "Epoch: 90   MAPE 47.44624\n",
      "sum_mpape:  42.89190284411112\n",
      "Epoch: 91   MAPE 47.47804\n",
      "sum_mpape:  42.74795433680217\n",
      "Epoch: 92   MAPE 47.59283\n",
      "sum_mpape:  42.62934006055196\n",
      "Epoch: 93   MAPE 47.63067\n",
      "sum_mpape:  42.443299277623495\n",
      "Epoch: 94   MAPE 47.42135\n",
      "sum_mpape:  42.3228036403656\n",
      "Epoch: 95   MAPE 47.299904\n",
      "sum_mpape:  42.17158765792847\n",
      "Epoch: 96   MAPE 47.079247\n",
      "sum_mpape:  42.08388867378235\n",
      "Epoch: 97   MAPE 47.40467\n",
      "sum_mpape:  42.07916000684102\n",
      "Epoch: 98   MAPE 47.441494\n",
      "sum_mpape:  42.00765581130982\n",
      "Epoch: 99   MAPE 47.721127\n",
      "sum_mpape:  41.924464829762776\n",
      "Epoch: 100   MAPE 47.45576\n",
      "sum_mpape:  41.77912182807923\n",
      "Epoch: 101   MAPE 47.821465\n",
      "sum_mpape:  41.77701403299967\n",
      "Epoch: 102   MAPE 47.87227\n",
      "sum_mpape:  41.60012822151184\n",
      "Epoch: 103   MAPE 47.81205\n",
      "sum_mpape:  41.55460756619771\n",
      "Epoch: 104   MAPE 48.056633\n",
      "sum_mpape:  41.75148261388143\n",
      "Epoch: 105   MAPE 48.07255\n",
      "sum_mpape:  41.590507634480794\n",
      "Epoch: 106   MAPE 48.055115\n",
      "sum_mpape:  41.36234660148621\n",
      "Epoch: 107   MAPE 48.160942\n",
      "sum_mpape:  41.26333576838176\n",
      "Epoch: 108   MAPE 48.711193\n",
      "sum_mpape:  41.13787126541138\n",
      "Epoch: 109   MAPE 49.17738\n",
      "sum_mpape:  41.693252611160275\n",
      "Epoch: 110   MAPE 49.624413\n",
      "sum_mpape:  40.75569492975871\n",
      "Epoch: 111   MAPE 48.980675\n",
      "sum_mpape:  40.74519611199697\n",
      "Epoch: 112   MAPE 49.11091\n",
      "sum_mpape:  40.48076923688253\n",
      "Epoch: 113   MAPE 49.451385\n",
      "sum_mpape:  40.49685529073079\n",
      "Epoch: 114   MAPE 49.811993\n",
      "sum_mpape:  40.485779190063475\n",
      "Epoch: 115   MAPE 50.208588\n",
      "sum_mpape:  40.80535790125529\n",
      "Epoch: 116   MAPE 51.186356\n",
      "sum_mpape:  40.40957477887471\n",
      "Epoch: 117   MAPE 50.204018\n",
      "sum_mpape:  40.19176913102468\n",
      "Epoch: 118   MAPE 51.005775\n",
      "sum_mpape:  40.16687269210816\n",
      "Epoch: 119   MAPE 51.74685\n",
      "sum_mpape:  40.09432582060496\n",
      "Epoch: 120   MAPE 52.317772\n",
      "sum_mpape:  40.14979705810547\n",
      "Epoch: 121   MAPE 51.931084\n",
      "sum_mpape:  40.84005756378174\n",
      "Epoch: 122   MAPE 52.083588\n",
      "sum_mpape:  39.85864764849345\n",
      "Epoch: 123   MAPE 52.986374\n",
      "sum_mpape:  40.806508938471474\n",
      "Epoch: 124   MAPE 51.66532\n",
      "sum_mpape:  39.945723088582355\n",
      "Epoch: 125   MAPE 52.017838\n",
      "sum_mpape:  39.910254549980166\n",
      "Epoch: 126   MAPE 52.518078\n",
      "sum_mpape:  39.696028192838035\n",
      "Epoch: 127   MAPE 53.31154\n",
      "sum_mpape:  39.637041886647545\n",
      "Epoch: 128   MAPE 53.788673\n",
      "sum_mpape:  39.648376893997195\n",
      "Epoch: 129   MAPE 53.14984\n",
      "sum_mpape:  39.611615443229674\n",
      "Epoch: 130   MAPE 55.011604\n",
      "sum_mpape:  39.73518231709798\n",
      "Epoch: 131   MAPE 54.61467\n",
      "sum_mpape:  39.49259521961212\n",
      "Epoch: 132   MAPE 52.45343\n",
      "sum_mpape:  40.553571430842084\n",
      "Epoch: 133   MAPE 51.853203\n",
      "sum_mpape:  39.23741247653961\n",
      "Epoch: 134   MAPE 50.731636\n",
      "sum_mpape:  39.307541116078696\n",
      "Epoch: 135   MAPE 51.26806\n",
      "sum_mpape:  39.009216197331746\n",
      "Epoch: 136   MAPE 50.04499\n",
      "sum_mpape:  38.99110570748647\n",
      "Epoch: 137   MAPE 49.559994\n",
      "sum_mpape:  38.84801897207896\n",
      "Epoch: 138   MAPE 50.50445\n",
      "sum_mpape:  38.79258282184601\n",
      "Epoch: 139   MAPE 50.579906\n",
      "sum_mpape:  38.871587777137755\n",
      "Epoch: 140   MAPE 50.53218\n",
      "sum_mpape:  38.86239860057831\n",
      "Epoch: 141   MAPE 49.977123\n",
      "sum_mpape:  38.812733014424644\n",
      "Epoch: 142   MAPE 51.02488\n",
      "sum_mpape:  38.85346572399139\n",
      "Epoch: 143   MAPE 50.231472\n",
      "sum_mpape:  38.824225068092346\n",
      "Epoch: 144   MAPE 49.651833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum_mpape:  38.721429785092674\n",
      "Epoch: 145   MAPE 50.27945\n",
      "sum_mpape:  38.462616388003035\n",
      "Epoch: 146   MAPE 49.9763\n",
      "sum_mpape:  38.7913730541865\n",
      "Epoch: 147   MAPE 48.71179\n",
      "sum_mpape:  38.825009950002034\n",
      "Epoch: 148   MAPE 49.57739\n",
      "sum_mpape:  38.66506128311157\n",
      "Epoch: 149   MAPE 49.42627\n",
      "sum_mpape:  38.65980831782023\n",
      "Epoch: 150   MAPE 49.1286\n",
      "sum_mpape:  38.7143514474233\n",
      "Epoch: 151   MAPE 50.6591\n",
      "sum_mpape:  39.044642837842304\n",
      "Epoch: 152   MAPE 50.746075\n",
      "sum_mpape:  38.59942820072174\n",
      "Epoch: 153   MAPE 49.7922\n",
      "sum_mpape:  38.4282204469045\n",
      "Epoch: 154   MAPE 50.058586\n",
      "sum_mpape:  38.57911269664764\n",
      "Epoch: 155   MAPE 50.823845\n",
      "sum_mpape:  38.37860809167226\n",
      "Epoch: 156   MAPE 50.973705\n",
      "sum_mpape:  38.43101101716359\n",
      "Epoch: 157   MAPE 50.173504\n",
      "sum_mpape:  38.30929468472799\n",
      "Epoch: 158   MAPE 50.22427\n",
      "sum_mpape:  38.33499173323313\n",
      "Epoch: 159   MAPE 49.482037\n",
      "sum_mpape:  38.11833102703095\n",
      "Epoch: 160   MAPE 50.563904\n",
      "sum_mpape:  38.184517065684\n",
      "Epoch: 161   MAPE 49.9131\n",
      "sum_mpape:  38.50777295430501\n",
      "Epoch: 162   MAPE 45.94651\n",
      "sum_mpape:  38.57427657445272\n",
      "Epoch: 163   MAPE 51.33188\n",
      "sum_mpape:  38.25814802646637\n",
      "Epoch: 164   MAPE 47.48762\n",
      "sum_mpape:  38.25499889055888\n",
      "Epoch: 165   MAPE 47.694088\n",
      "sum_mpape:  38.1671659151713\n",
      "Epoch: 166   MAPE 49.30847\n",
      "sum_mpape:  38.03666892846425\n",
      "Epoch: 167   MAPE 49.081726\n",
      "sum_mpape:  38.00221330324809\n",
      "Epoch: 168   MAPE 48.747437\n",
      "sum_mpape:  37.91164580186208\n",
      "Epoch: 169   MAPE 45.538036\n",
      "sum_mpape:  38.115474716822305\n",
      "Epoch: 170   MAPE 46.724487\n",
      "sum_mpape:  37.99762664635976\n",
      "Epoch: 171   MAPE 46.71734\n",
      "sum_mpape:  37.95977648099264\n",
      "Epoch: 172   MAPE 47.846046\n",
      "sum_mpape:  37.87549305756887\n",
      "Epoch: 173   MAPE 47.570793\n",
      "sum_mpape:  37.85276778539022\n",
      "Epoch: 174   MAPE 48.761642\n",
      "sum_mpape:  37.85011514027914\n",
      "Epoch: 175   MAPE 48.82222\n",
      "sum_mpape:  37.71643149058024\n",
      "Epoch: 176   MAPE 47.353592\n",
      "sum_mpape:  37.73181410630544\n",
      "Epoch: 177   MAPE 47.166016\n",
      "sum_mpape:  37.646094870567325\n",
      "Epoch: 178   MAPE 45.027557\n",
      "sum_mpape:  37.76258150736491\n",
      "Epoch: 179   MAPE 46.823544\n",
      "sum_mpape:  37.74843463102977\n",
      "Epoch: 180   MAPE 42.412933\n",
      "sum_mpape:  37.70928185780843\n",
      "Epoch: 181   MAPE 42.38806\n",
      "sum_mpape:  37.640358646710716\n",
      "Epoch: 182   MAPE 46.283535\n",
      "sum_mpape:  37.58535955746969\n",
      "Epoch: 183   MAPE 44.34932\n",
      "sum_mpape:  37.64173645178477\n",
      "Epoch: 184   MAPE 43.926735\n",
      "sum_mpape:  37.643519115448\n",
      "Epoch: 185   MAPE 41.931217\n",
      "sum_mpape:  37.5011819044749\n",
      "Epoch: 186   MAPE 45.54541\n",
      "sum_mpape:  37.78672411441803\n",
      "Epoch: 187   MAPE 43.197502\n",
      "sum_mpape:  37.51099272569021\n",
      "Epoch: 188   MAPE 43.957317\n",
      "sum_mpape:  37.52731593449911\n",
      "Epoch: 189   MAPE 42.53006\n",
      "sum_mpape:  37.18865258693695\n",
      "Epoch: 190   MAPE 42.489685\n",
      "sum_mpape:  37.01288676261902\n",
      "Epoch: 191   MAPE 44.88137\n",
      "sum_mpape:  36.91815600395203\n",
      "Epoch: 192   MAPE 41.716877\n",
      "sum_mpape:  36.777616532643634\n",
      "Epoch: 193   MAPE 42.831203\n",
      "sum_mpape:  36.826630266507465\n",
      "Epoch: 194   MAPE 42.468464\n",
      "sum_mpape:  36.84380949338277\n",
      "Epoch: 195   MAPE 41.407402\n",
      "sum_mpape:  36.80799547036489\n",
      "Epoch: 196   MAPE 41.790627\n",
      "sum_mpape:  36.82033644517263\n",
      "Epoch: 197   MAPE 54.39782\n",
      "sum_mpape:  37.14363512992859\n",
      "Epoch: 198   MAPE 42.495163\n",
      "sum_mpape:  36.524007113774616\n",
      "Epoch: 199   MAPE 41.96618\n",
      "sum_mpape:  36.86959673563639\n",
      "Epoch: 200   MAPE 41.59578\n",
      "sum_mpape:  40.08432230949402\n",
      "Epoch: 201   MAPE 44.048504\n",
      "sum_mpape:  40.33962966601054\n",
      "Epoch: 202   MAPE 43.887436\n",
      "sum_mpape:  40.0184331258138\n",
      "Epoch: 203   MAPE 43.659523\n",
      "sum_mpape:  39.78752905527751\n",
      "Epoch: 204   MAPE 43.484886\n",
      "sum_mpape:  39.62206505139669\n",
      "Epoch: 205   MAPE 43.649025\n",
      "sum_mpape:  39.41277516682943\n",
      "Epoch: 206   MAPE 43.48762\n",
      "sum_mpape:  39.306918533643085\n",
      "Epoch: 207   MAPE 43.641293\n",
      "sum_mpape:  39.10988138516744\n",
      "Epoch: 208   MAPE 43.91433\n",
      "sum_mpape:  38.98016614913941\n",
      "Epoch: 209   MAPE 43.779247\n",
      "sum_mpape:  38.76131985187531\n",
      "Epoch: 210   MAPE 44.053772\n",
      "sum_mpape:  38.76684013207753\n",
      "Epoch: 211   MAPE 45.299767\n",
      "sum_mpape:  38.81412796974182\n",
      "Epoch: 212   MAPE 44.87507\n",
      "sum_mpape:  39.63081903457642\n",
      "Epoch: 213   MAPE 42.20809\n",
      "sum_mpape:  39.67139644622803\n",
      "Epoch: 214   MAPE 43.329254\n",
      "sum_mpape:  39.49117482503255\n",
      "Epoch: 215   MAPE 43.08689\n",
      "sum_mpape:  39.35108480453491\n",
      "Epoch: 216   MAPE 43.527\n",
      "sum_mpape:  39.300509532292686\n",
      "Epoch: 217   MAPE 44.1589\n",
      "sum_mpape:  39.6315714041392\n",
      "Epoch: 218   MAPE 44.16435\n",
      "sum_mpape:  39.25903576215108\n",
      "Epoch: 219   MAPE 44.273857\n",
      "sum_mpape:  39.39040608406067\n",
      "Epoch: 220   MAPE 42.261196\n",
      "sum_mpape:  39.38249926567077\n",
      "Epoch: 221   MAPE 42.96484\n",
      "sum_mpape:  38.83690601189931\n",
      "Epoch: 222   MAPE 45.527252\n",
      "sum_mpape:  39.52608680725098\n",
      "Epoch: 223   MAPE 42.947918\n",
      "sum_mpape:  40.39819625218709\n",
      "Epoch: 224   MAPE 45.683556\n",
      "sum_mpape:  38.397786593437196\n",
      "Epoch: 225   MAPE 42.27367\n",
      "sum_mpape:  37.55693907737732\n",
      "Epoch: 226   MAPE 41.81712\n",
      "sum_mpape:  37.26250537236532\n",
      "Epoch: 227   MAPE 41.86595\n",
      "sum_mpape:  37.100001533826195\n",
      "Epoch: 228   MAPE 41.77465\n",
      "sum_mpape:  36.91086328824361\n",
      "Epoch: 229   MAPE 40.97473\n",
      "sum_mpape:  36.54269239107768\n",
      "Epoch: 230   MAPE 41.05227\n",
      "sum_mpape:  36.30350210666656\n",
      "Epoch: 231   MAPE 41.059956\n",
      "sum_mpape:  36.13105250994364\n",
      "Epoch: 232   MAPE 41.228714\n",
      "sum_mpape:  35.89195109208425\n",
      "Epoch: 233   MAPE 41.012184\n",
      "sum_mpape:  35.34358781973521\n",
      "Epoch: 234   MAPE 40.12798\n",
      "sum_mpape:  34.895531209309894\n",
      "Epoch: 235   MAPE 40.128944\n",
      "sum_mpape:  34.472835604349775\n",
      "Epoch: 236   MAPE 39.944767\n",
      "sum_mpape:  34.160103901227316\n",
      "Epoch: 237   MAPE 39.367123\n",
      "sum_mpape:  33.94479049841563\n",
      "Epoch: 238   MAPE 38.91871\n",
      "sum_mpape:  33.658761604626974\n",
      "Epoch: 239   MAPE 38.305782\n",
      "sum_mpape:  33.496033906936646\n",
      "Epoch: 240   MAPE 38.299038\n",
      "sum_mpape:  33.2655314763387\n",
      "Epoch: 241   MAPE 37.87377\n",
      "sum_mpape:  32.97209500471751\n",
      "Epoch: 242   MAPE 38.03519\n",
      "sum_mpape:  32.569856278101604\n",
      "Epoch: 243   MAPE 38.162895\n",
      "sum_mpape:  32.32772971789042\n",
      "Epoch: 244   MAPE 37.628304\n",
      "sum_mpape:  32.12132425308228\n",
      "Epoch: 245   MAPE 37.431118\n",
      "sum_mpape:  31.95429819424947\n",
      "Epoch: 246   MAPE 37.4332\n",
      "sum_mpape:  31.852764097849526\n",
      "Epoch: 247   MAPE 36.977806\n",
      "sum_mpape:  32.184308584531145\n",
      "Epoch: 248   MAPE 36.838593\n",
      "sum_mpape:  32.20002528826396\n",
      "Epoch: 249   MAPE 36.828987\n",
      "sum_mpape:  31.882576044400533\n",
      "Epoch: 250   MAPE 36.781517\n",
      "sum_mpape:  32.01768724918365\n",
      "Epoch: 251   MAPE 36.876503\n",
      "sum_mpape:  31.97421526114146\n",
      "Epoch: 252   MAPE 37.45828\n",
      "sum_mpape:  31.86489545504252\n",
      "Epoch: 253   MAPE 36.819237\n",
      "sum_mpape:  31.74408503373464\n",
      "Epoch: 254   MAPE 36.87314\n",
      "sum_mpape:  31.557801723480225\n",
      "Epoch: 255   MAPE 36.777725\n",
      "sum_mpape:  31.28826318581899\n",
      "Epoch: 256   MAPE 36.389412\n",
      "sum_mpape:  31.389183417956033\n",
      "Epoch: 257   MAPE 36.33691\n",
      "sum_mpape:  31.35620334148407\n",
      "Epoch: 258   MAPE 36.025024\n",
      "sum_mpape:  31.030720265706382\n",
      "Epoch: 259   MAPE 35.826817\n",
      "sum_mpape:  30.9151637951533\n",
      "Epoch: 260   MAPE 35.57843\n",
      "sum_mpape:  30.89911992549896\n",
      "Epoch: 261   MAPE 35.800243\n",
      "sum_mpape:  30.833329518636067\n",
      "Epoch: 262   MAPE 35.67605\n",
      "sum_mpape:  30.792193818092347\n",
      "Epoch: 263   MAPE 35.56986\n",
      "sum_mpape:  30.819760632514953\n",
      "Epoch: 264   MAPE 35.24725\n",
      "sum_mpape:  30.958992139498392\n",
      "Epoch: 265   MAPE 36.280365\n",
      "sum_mpape:  31.463529578844707\n",
      "Epoch: 266   MAPE 34.576035\n",
      "sum_mpape:  30.679102484385172\n",
      "Epoch: 267   MAPE 35.299683\n",
      "sum_mpape:  30.478330373764038\n",
      "Epoch: 268   MAPE 35.153564\n",
      "sum_mpape:  30.688203859329224\n",
      "Epoch: 269   MAPE 34.461117\n",
      "sum_mpape:  30.638951953252157\n",
      "Epoch: 270   MAPE 34.63802\n",
      "sum_mpape:  30.37397967974345\n",
      "Epoch: 271   MAPE 35.343124\n",
      "sum_mpape:  30.909696261088055\n",
      "Epoch: 272   MAPE 33.328888\n",
      "sum_mpape:  30.698797710736592\n",
      "Epoch: 273   MAPE 33.9989\n",
      "sum_mpape:  30.475157165527342\n",
      "Epoch: 274   MAPE 33.774136\n",
      "sum_mpape:  30.23646474679311\n",
      "Epoch: 275   MAPE 33.34238\n",
      "sum_mpape:  30.04513345559438\n",
      "Epoch: 276   MAPE 33.18779\n",
      "sum_mpape:  29.885030150413513\n",
      "Epoch: 277   MAPE 33.669567\n",
      "sum_mpape:  29.863230601946512\n",
      "Epoch: 278   MAPE 33.05474\n",
      "sum_mpape:  29.80142281850179\n",
      "Epoch: 279   MAPE 32.774887\n",
      "sum_mpape:  29.60356489022573\n",
      "Epoch: 280   MAPE 32.444313\n",
      "sum_mpape:  29.5795769850413\n",
      "Epoch: 281   MAPE 31.335186\n",
      "sum_mpape:  29.31500816345215\n",
      "Epoch: 282   MAPE 31.297989\n",
      "sum_mpape:  29.214272141456604\n",
      "Epoch: 283   MAPE 31.390877\n",
      "sum_mpape:  29.20675988992055\n",
      "Epoch: 284   MAPE 31.537731\n",
      "sum_mpape:  29.050551040967306\n",
      "Epoch: 285   MAPE 31.30775\n",
      "sum_mpape:  29.01786208152771\n",
      "Epoch: 286   MAPE 31.048315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum_mpape:  29.686501185099285\n",
      "Epoch: 287   MAPE 33.112274\n",
      "sum_mpape:  30.123866526285806\n",
      "Epoch: 288   MAPE 32.46126\n",
      "sum_mpape:  29.698764578501383\n",
      "Epoch: 289   MAPE 32.235992\n",
      "sum_mpape:  30.06750196615855\n",
      "Epoch: 290   MAPE 31.328651\n",
      "sum_mpape:  31.208383146921793\n",
      "Epoch: 291   MAPE 30.280796\n",
      "sum_mpape:  30.534521063168842\n",
      "Epoch: 292   MAPE 30.779533\n",
      "sum_mpape:  30.010034489631654\n",
      "Epoch: 293   MAPE 31.011856\n",
      "sum_mpape:  29.726289335886637\n",
      "Epoch: 294   MAPE 31.146225\n",
      "sum_mpape:  29.77938459714254\n",
      "Epoch: 295   MAPE 30.765457\n",
      "sum_mpape:  31.22397372722626\n",
      "Epoch: 296   MAPE 29.99982\n",
      "sum_mpape:  29.712166770299277\n",
      "Epoch: 297   MAPE 30.301147\n",
      "sum_mpape:  29.28126360575358\n",
      "Epoch: 298   MAPE 30.941761\n",
      "sum_mpape:  29.242045704523722\n",
      "Epoch: 299   MAPE 30.91223\n",
      "sum_mpape:  29.16640216509501\n",
      "Epoch: 300   MAPE 31.37792\n",
      "sum_mpape:  28.972971693674722\n",
      "Epoch: 301   MAPE 31.45287\n",
      "sum_mpape:  29.091223057111105\n",
      "Epoch: 302   MAPE 31.430904\n",
      "sum_mpape:  28.887386695543924\n",
      "Epoch: 303   MAPE 31.117874\n",
      "sum_mpape:  28.790338683128358\n",
      "Epoch: 304   MAPE 31.500221\n",
      "sum_mpape:  28.730811675389607\n",
      "Epoch: 305   MAPE 31.18434\n",
      "sum_mpape:  28.696305346488952\n",
      "Epoch: 306   MAPE 30.964743\n",
      "sum_mpape:  28.650278107325235\n",
      "Epoch: 307   MAPE 31.134483\n",
      "sum_mpape:  28.62734091281891\n",
      "Epoch: 308   MAPE 31.012388\n",
      "sum_mpape:  30.020260802904765\n",
      "Epoch: 309   MAPE 35.551125\n",
      "sum_mpape:  30.486899216969807\n",
      "Epoch: 310   MAPE 32.56716\n",
      "sum_mpape:  30.065803480148315\n",
      "Epoch: 311   MAPE 32.315147\n",
      "sum_mpape:  29.882899030049643\n",
      "Epoch: 312   MAPE 32.263317\n",
      "sum_mpape:  29.748638558387757\n",
      "Epoch: 313   MAPE 31.75573\n",
      "sum_mpape:  29.588242467244466\n",
      "Epoch: 314   MAPE 31.524656\n",
      "sum_mpape:  29.38923223018646\n",
      "Epoch: 315   MAPE 31.542141\n",
      "sum_mpape:  29.884708015124\n",
      "Epoch: 316   MAPE 32.381645\n",
      "sum_mpape:  29.701138297716778\n",
      "Epoch: 317   MAPE 32.16275\n",
      "sum_mpape:  29.351117340723672\n",
      "Epoch: 318   MAPE 31.994444\n",
      "sum_mpape:  29.313866504033406\n",
      "Epoch: 319   MAPE 31.901936\n",
      "sum_mpape:  29.24704307715098\n",
      "Epoch: 320   MAPE 32.032234\n",
      "sum_mpape:  29.475052658716837\n",
      "Epoch: 321   MAPE 32.142136\n",
      "sum_mpape:  28.857265615463255\n",
      "Epoch: 322   MAPE 32.03385\n",
      "sum_mpape:  28.801930371920268\n",
      "Epoch: 323   MAPE 32.306866\n",
      "sum_mpape:  28.727255868911744\n",
      "Epoch: 324   MAPE 32.247375\n",
      "sum_mpape:  28.53470764954885\n",
      "Epoch: 325   MAPE 31.810654\n",
      "sum_mpape:  28.462250510851543\n",
      "Epoch: 326   MAPE 31.688505\n",
      "sum_mpape:  28.398930994669595\n",
      "Epoch: 327   MAPE 32.13196\n",
      "sum_mpape:  28.372664507230123\n",
      "Epoch: 328   MAPE 31.640776\n",
      "sum_mpape:  28.295440769195558\n",
      "Epoch: 329   MAPE 31.729107\n",
      "sum_mpape:  28.262423714001972\n",
      "Epoch: 330   MAPE 31.872402\n",
      "sum_mpape:  28.139581688245137\n",
      "Epoch: 331   MAPE 31.725433\n",
      "sum_mpape:  28.104760511716208\n",
      "Epoch: 332   MAPE 31.674673\n",
      "sum_mpape:  28.031251724561056\n",
      "Epoch: 333   MAPE 31.66869\n",
      "sum_mpape:  27.99792066415151\n",
      "Epoch: 334   MAPE 31.621206\n",
      "sum_mpape:  27.958890771865846\n",
      "Epoch: 335   MAPE 31.353031\n",
      "sum_mpape:  27.924052397410076\n",
      "Epoch: 336   MAPE 31.35851\n",
      "sum_mpape:  28.398785463968913\n",
      "Epoch: 337   MAPE 32.18367\n",
      "sum_mpape:  28.94253426392873\n",
      "Epoch: 338   MAPE 32.91052\n",
      "sum_mpape:  28.95252462228139\n",
      "Epoch: 339   MAPE 32.208893\n",
      "sum_mpape:  28.6228493531545\n",
      "Epoch: 340   MAPE 32.083508\n",
      "sum_mpape:  28.634627230962117\n",
      "Epoch: 341   MAPE 32.030464\n",
      "sum_mpape:  28.620794264475503\n",
      "Epoch: 342   MAPE 31.051153\n",
      "sum_mpape:  31.150490625699362\n",
      "Epoch: 343   MAPE 29.764313\n",
      "sum_mpape:  29.46197865009308\n",
      "Epoch: 344   MAPE 32.82583\n",
      "sum_mpape:  28.542489329973858\n",
      "Epoch: 345   MAPE 31.699202\n",
      "sum_mpape:  28.2832004626592\n",
      "Epoch: 346   MAPE 31.346767\n",
      "sum_mpape:  28.271409138043722\n",
      "Epoch: 347   MAPE 31.508972\n",
      "sum_mpape:  28.14984631538391\n",
      "Epoch: 348   MAPE 31.336882\n",
      "sum_mpape:  28.239103587468467\n",
      "Epoch: 349   MAPE 31.395107\n",
      "sum_mpape:  27.983021179835003\n",
      "Epoch: 350   MAPE 31.142681\n",
      "sum_mpape:  27.967134229342143\n",
      "Epoch: 351   MAPE 31.003956\n",
      "sum_mpape:  27.887986922264098\n",
      "Epoch: 352   MAPE 30.701315\n",
      "sum_mpape:  27.804448692003884\n",
      "Epoch: 353   MAPE 30.802013\n",
      "sum_mpape:  27.85258384545644\n",
      "Epoch: 354   MAPE 30.505543\n",
      "sum_mpape:  27.77054096062978\n",
      "Epoch: 355   MAPE 30.749228\n",
      "sum_mpape:  27.72896018028259\n",
      "Epoch: 356   MAPE 30.707083\n",
      "sum_mpape:  27.72842966715495\n",
      "Epoch: 357   MAPE 30.62336\n",
      "sum_mpape:  27.65077959696452\n",
      "Epoch: 358   MAPE 30.751144\n",
      "sum_mpape:  27.618794377644857\n",
      "Epoch: 359   MAPE 30.491135\n",
      "sum_mpape:  27.539465284347536\n",
      "Epoch: 360   MAPE 30.74094\n",
      "sum_mpape:  27.554392925898235\n",
      "Epoch: 361   MAPE 30.759552\n",
      "sum_mpape:  27.520101976394653\n",
      "Epoch: 362   MAPE 30.59055\n",
      "sum_mpape:  27.580491105715435\n",
      "Epoch: 363   MAPE 30.41935\n",
      "sum_mpape:  27.46768229007721\n",
      "Epoch: 364   MAPE 30.411564\n",
      "sum_mpape:  27.52873251438141\n",
      "Epoch: 365   MAPE 30.315971\n",
      "sum_mpape:  27.468633770942688\n",
      "Epoch: 366   MAPE 30.190926\n",
      "sum_mpape:  27.485773499806722\n",
      "Epoch: 367   MAPE 29.93176\n",
      "sum_mpape:  27.627672235171\n",
      "Epoch: 368   MAPE 30.370697\n",
      "sum_mpape:  27.75538444519043\n",
      "Epoch: 369   MAPE 30.197088\n",
      "sum_mpape:  27.447038952509562\n",
      "Epoch: 370   MAPE 30.23671\n",
      "sum_mpape:  27.66037579377492\n",
      "Epoch: 371   MAPE 30.057554\n",
      "sum_mpape:  27.468267583847044\n",
      "Epoch: 372   MAPE 30.02432\n",
      "sum_mpape:  27.527491466204324\n",
      "Epoch: 373   MAPE 29.84262\n",
      "sum_mpape:  27.32898584206899\n",
      "Epoch: 374   MAPE 29.817696\n",
      "sum_mpape:  27.34443097114563\n",
      "Epoch: 375   MAPE 29.830627\n",
      "sum_mpape:  27.24495623111725\n",
      "Epoch: 376   MAPE 30.061499\n",
      "sum_mpape:  27.151149288813272\n",
      "Epoch: 377   MAPE 30.05123\n",
      "sum_mpape:  27.13209946155548\n",
      "Epoch: 378   MAPE 29.639141\n",
      "sum_mpape:  27.241249871253967\n",
      "Epoch: 379   MAPE 29.847988\n",
      "sum_mpape:  27.31314947605133\n",
      "Epoch: 380   MAPE 29.786963\n",
      "sum_mpape:  27.19577683607737\n",
      "Epoch: 381   MAPE 29.667038\n",
      "sum_mpape:  27.106482601165773\n",
      "Epoch: 382   MAPE 29.421614\n",
      "sum_mpape:  27.141107964515687\n",
      "Epoch: 383   MAPE 29.446676\n",
      "sum_mpape:  27.10063610871633\n",
      "Epoch: 384   MAPE 29.54447\n",
      "sum_mpape:  27.120255398750306\n",
      "Epoch: 385   MAPE 29.451796\n",
      "sum_mpape:  27.056029510498046\n",
      "Epoch: 386   MAPE 29.320135\n",
      "sum_mpape:  27.052960991859436\n",
      "Epoch: 387   MAPE 29.148712\n",
      "sum_mpape:  27.019527777036032\n",
      "Epoch: 388   MAPE 29.34829\n",
      "sum_mpape:  27.004780960083007\n",
      "Epoch: 389   MAPE 29.32743\n",
      "sum_mpape:  27.042125423749287\n",
      "Epoch: 390   MAPE 29.236713\n",
      "sum_mpape:  26.990170884132386\n",
      "Epoch: 391   MAPE 29.02934\n",
      "sum_mpape:  27.008766428629556\n",
      "Epoch: 392   MAPE 29.334494\n",
      "sum_mpape:  26.908052810033162\n",
      "Epoch: 393   MAPE 28.854645\n",
      "sum_mpape:  26.74023325840632\n",
      "Epoch: 394   MAPE 29.128666\n",
      "sum_mpape:  26.782909154891968\n",
      "Epoch: 395   MAPE 28.81877\n",
      "sum_mpape:  26.771101649602254\n",
      "Epoch: 396   MAPE 29.336166\n",
      "sum_mpape:  26.692009965578716\n",
      "Epoch: 397   MAPE 28.99912\n",
      "sum_mpape:  26.700764620304106\n",
      "Epoch: 398   MAPE 28.671333\n",
      "sum_mpape:  26.71647158463796\n",
      "Epoch: 399   MAPE 28.853344\n",
      "sum_mpape:  26.66504379113515\n",
      "Epoch: 400   MAPE 28.758062\n",
      "sum_mpape:  26.599794340133666\n",
      "Epoch: 401   MAPE 28.830456\n",
      "sum_mpape:  26.848286183675132\n",
      "Epoch: 402   MAPE 29.152252\n",
      "sum_mpape:  26.845887303352356\n",
      "Epoch: 403   MAPE 28.839664\n",
      "sum_mpape:  26.78473610083262\n",
      "Epoch: 404   MAPE 28.83595\n",
      "sum_mpape:  26.66139211654663\n",
      "Epoch: 405   MAPE 28.607023\n",
      "sum_mpape:  26.644427982966103\n",
      "Epoch: 406   MAPE 28.536041\n",
      "sum_mpape:  26.827604500452676\n",
      "Epoch: 407   MAPE 28.815033\n",
      "sum_mpape:  26.78431599934896\n",
      "Epoch: 408   MAPE 28.761879\n",
      "sum_mpape:  26.616189384460448\n",
      "Epoch: 409   MAPE 28.584393\n",
      "sum_mpape:  26.60850534439087\n",
      "Epoch: 410   MAPE 28.387579\n",
      "sum_mpape:  26.695211863517763\n",
      "Epoch: 411   MAPE 28.237904\n",
      "sum_mpape:  26.542716654141746\n",
      "Epoch: 412   MAPE 28.531477\n",
      "sum_mpape:  26.50990982055664\n",
      "Epoch: 413   MAPE 28.51617\n",
      "sum_mpape:  26.502550959587097\n",
      "Epoch: 414   MAPE 28.221792\n",
      "sum_mpape:  26.48318007787069\n",
      "Epoch: 415   MAPE 28.665106\n",
      "sum_mpape:  26.484890778859455\n",
      "Epoch: 416   MAPE 28.279903\n",
      "sum_mpape:  26.45269686381022\n",
      "Epoch: 417   MAPE 28.43652\n",
      "sum_mpape:  26.511898231506347\n",
      "Epoch: 418   MAPE 27.956036\n",
      "sum_mpape:  26.451443441708882\n",
      "Epoch: 419   MAPE 27.808983\n",
      "sum_mpape:  26.35775299866994\n",
      "Epoch: 420   MAPE 27.971748\n",
      "sum_mpape:  26.539968911806742\n",
      "Epoch: 421   MAPE 28.07648\n",
      "sum_mpape:  26.810193014144897\n",
      "Epoch: 422   MAPE 35.055832\n",
      "sum_mpape:  26.868765397866568\n",
      "Epoch: 423   MAPE 29.384396\n",
      "sum_mpape:  26.972114503383636\n",
      "Epoch: 424   MAPE 29.094418\n",
      "sum_mpape:  26.890101742744445\n",
      "Epoch: 425   MAPE 27.82871\n",
      "sum_mpape:  26.570012084643047\n",
      "Epoch: 426   MAPE 28.760344\n",
      "sum_mpape:  26.51503612200419\n",
      "Epoch: 427   MAPE 27.744879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum_mpape:  26.453866291046143\n",
      "Epoch: 428   MAPE 27.765137\n",
      "sum_mpape:  26.47576138575872\n",
      "Epoch: 429   MAPE 27.4632\n",
      "sum_mpape:  26.372462836901345\n",
      "Epoch: 430   MAPE 27.921432\n",
      "sum_mpape:  26.341869763533275\n",
      "Epoch: 431   MAPE 27.763817\n",
      "sum_mpape:  26.30457765261332\n",
      "Epoch: 432   MAPE 27.514957\n",
      "sum_mpape:  26.302937920888265\n",
      "Epoch: 433   MAPE 27.646078\n",
      "sum_mpape:  26.25602417786916\n",
      "Epoch: 434   MAPE 27.526617\n",
      "sum_mpape:  26.16420431534449\n",
      "Epoch: 435   MAPE 27.447834\n",
      "sum_mpape:  26.200762518246968\n",
      "Epoch: 436   MAPE 27.540289\n",
      "sum_mpape:  26.228417122364043\n",
      "Epoch: 437   MAPE 27.388988\n",
      "sum_mpape:  26.217111444473268\n",
      "Epoch: 438   MAPE 27.404623\n",
      "sum_mpape:  26.05965623060862\n",
      "Epoch: 439   MAPE 27.343006\n",
      "sum_mpape:  26.10418742497762\n",
      "Epoch: 440   MAPE 27.523977\n",
      "sum_mpape:  26.21851682662964\n",
      "Epoch: 441   MAPE 27.238937\n",
      "sum_mpape:  26.019590202967326\n",
      "Epoch: 442   MAPE 27.552626\n",
      "sum_mpape:  26.04269288778305\n",
      "Epoch: 443   MAPE 27.374058\n",
      "sum_mpape:  26.048266665140787\n",
      "Epoch: 444   MAPE 27.573948\n",
      "sum_mpape:  26.023109463850655\n",
      "Epoch: 445   MAPE 27.652388\n",
      "sum_mpape:  26.01589548587799\n",
      "Epoch: 446   MAPE 27.72714\n",
      "sum_mpape:  25.965359302361808\n",
      "Epoch: 447   MAPE 27.700474\n",
      "sum_mpape:  25.927253794670104\n",
      "Epoch: 448   MAPE 27.446661\n",
      "sum_mpape:  26.217486266295115\n",
      "Epoch: 449   MAPE 27.438065\n",
      "sum_mpape:  26.117476197083793\n",
      "Epoch: 450   MAPE 27.446478\n",
      "sum_mpape:  26.09032032887141\n",
      "Epoch: 451   MAPE 27.4379\n",
      "sum_mpape:  26.02709806362788\n",
      "Epoch: 452   MAPE 27.22525\n",
      "sum_mpape:  25.962743147214255\n",
      "Epoch: 453   MAPE 26.945251\n",
      "sum_mpape:  25.869230433305106\n",
      "Epoch: 454   MAPE 27.182655\n",
      "sum_mpape:  25.83236261208852\n",
      "Epoch: 455   MAPE 27.036093\n",
      "sum_mpape:  25.93331489165624\n",
      "Epoch: 456   MAPE 27.26331\n",
      "sum_mpape:  25.90485416650772\n",
      "Epoch: 457   MAPE 27.313704\n",
      "sum_mpape:  25.9346031665802\n",
      "Epoch: 458   MAPE 27.438847\n",
      "sum_mpape:  26.041061639785767\n",
      "Epoch: 459   MAPE 28.560146\n",
      "sum_mpape:  25.877629431088767\n",
      "Epoch: 460   MAPE 27.26782\n",
      "sum_mpape:  25.82740794022878\n",
      "Epoch: 461   MAPE 27.007694\n",
      "sum_mpape:  25.789124453067778\n",
      "Epoch: 462   MAPE 27.317022\n",
      "sum_mpape:  25.867792205015817\n",
      "Epoch: 463   MAPE 27.304111\n",
      "sum_mpape:  25.82837473154068\n",
      "Epoch: 464   MAPE 27.287107\n",
      "sum_mpape:  25.911481658617657\n",
      "Epoch: 465   MAPE 27.266216\n",
      "sum_mpape:  25.82507735490799\n",
      "Epoch: 466   MAPE 27.363514\n",
      "sum_mpape:  25.764049633344015\n",
      "Epoch: 467   MAPE 26.994307\n",
      "sum_mpape:  25.737890215714774\n",
      "Epoch: 468   MAPE 26.922066\n",
      "sum_mpape:  25.74651571114858\n",
      "Epoch: 469   MAPE 26.78339\n",
      "sum_mpape:  25.662913743654887\n",
      "Epoch: 470   MAPE 26.954191\n",
      "sum_mpape:  25.687497067451478\n",
      "Epoch: 471   MAPE 26.913292\n",
      "sum_mpape:  25.752522166570028\n",
      "Epoch: 472   MAPE 26.697048\n",
      "sum_mpape:  25.77440591653188\n",
      "Epoch: 473   MAPE 26.981815\n",
      "sum_mpape:  25.742441129684448\n",
      "Epoch: 474   MAPE 26.613575\n",
      "sum_mpape:  25.670592800776163\n",
      "Epoch: 475   MAPE 26.991575\n",
      "sum_mpape:  25.590886664390563\n",
      "Epoch: 476   MAPE 26.877884\n",
      "sum_mpape:  25.649176398913067\n",
      "Epoch: 477   MAPE 26.686916\n",
      "sum_mpape:  25.615213429927827\n",
      "Epoch: 478   MAPE 26.594822\n",
      "sum_mpape:  25.78451596101125\n",
      "Epoch: 479   MAPE 26.637564\n",
      "sum_mpape:  25.9732851823171\n",
      "Epoch: 480   MAPE 27.386333\n",
      "sum_mpape:  25.896305282910664\n",
      "Epoch: 481   MAPE 26.396236\n",
      "sum_mpape:  25.73265440861384\n",
      "Epoch: 482   MAPE 26.427715\n",
      "sum_mpape:  25.58576498826345\n",
      "Epoch: 483   MAPE 26.580145\n",
      "sum_mpape:  25.563244060675302\n",
      "Epoch: 484   MAPE 26.463924\n",
      "sum_mpape:  25.700219666957857\n",
      "Epoch: 485   MAPE 26.287838\n",
      "sum_mpape:  25.623201151688892\n",
      "Epoch: 486   MAPE 26.322605\n",
      "sum_mpape:  25.658144378662108\n",
      "Epoch: 487   MAPE 26.463469\n",
      "sum_mpape:  25.599630351861318\n",
      "Epoch: 488   MAPE 26.507465\n",
      "sum_mpape:  25.58939161300659\n",
      "Epoch: 489   MAPE 26.58653\n",
      "sum_mpape:  25.462145686149597\n",
      "Epoch: 490   MAPE 26.243713\n",
      "sum_mpape:  25.548687279224396\n",
      "Epoch: 491   MAPE 26.241156\n",
      "sum_mpape:  26.24833475748698\n",
      "Epoch: 492   MAPE 27.326157\n",
      "sum_mpape:  25.714102101325988\n",
      "Epoch: 493   MAPE 26.10981\n",
      "sum_mpape:  25.69188894033432\n",
      "Epoch: 494   MAPE 26.614723\n",
      "sum_mpape:  25.45022069613139\n",
      "Epoch: 495   MAPE 26.052448\n",
      "sum_mpape:  25.585961572329204\n",
      "Epoch: 496   MAPE 26.883137\n",
      "sum_mpape:  25.581902678807577\n",
      "Epoch: 497   MAPE 26.288824\n",
      "sum_mpape:  25.45698745648066\n",
      "Epoch: 498   MAPE 25.951262\n",
      "sum_mpape:  25.518165616194405\n",
      "Epoch: 499   MAPE 25.97975\n",
      "sum_mpape:  25.485349237918854\n"
     ]
    }
   ],
   "source": [
    "Pred_ContextL = [] \n",
    "Pred_ResultL= []\n",
    "Output_Seq_L = []\n",
    "Context_embedL = []\n",
    "        \n",
    "for epoch in range(500):\n",
    "    min_mape = 50\n",
    "    sum_mpape = 0\n",
    "    for i in range(batch_number):\n",
    "        input1,input2,real_label = get_batch_data(batch_size,i)\n",
    "        #print(input1,input2)\n",
    "        feed_dict={graph['x1']:input1,graph['x2']:input2,graph['Output_Seq']:real_label}\n",
    "        _,Pred_Context_out,Pred_Result,loss2,lossFC,ADJ_matrix,Output_seq,loss_shape = sess.run([graph['train'],graph['Pred_Context_out'],graph['Pred_Result'],graph['loss2'],graph['loss_FC'],graph['ADJ_matrix'],graph['Output_Seq'],graph['loss_shape']],feed_dict)\n",
    "        sum_mpape = sum_mpape + loss2\n",
    "        #_,loss1,Pred_Context_out,Pred_Result,Output_Seq,context_embed, loss2,lossFC,ADJ_matrix= sess.run([graph['train'],graph['loss1'],graph['Pred_Context_out'],graph['Pred_Result'],graph['Output_Seq'],graph['context_embed'],graph['loss2'],graph['loss_FC'],graph['ADJ_matrix']],feed_dict)\n",
    "        if(i%240==0):\n",
    "            #print(  \"MSE:\",loss1,\"  MAPE\",loss2,\"预测结果:\",Pred_Result[0],\" 目标: \",real_label[:,0,:])\n",
    "            #print( \"Epoch:\", epoch,  \"  MAPE\",loss2,\"Conxtloss\", lossFC,\"loss_shape:\", loss_shape,\" Pred_result:\", Pred_Result[0][:,15],Output_seq[0][:,15])\n",
    "            print( \"Epoch:\", epoch,  \"  MAPE\",loss2)\n",
    "    sum_mpape = sum_mpape/batch_number\n",
    "    print(\"sum_mpape: \",sum_mpape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0   MAPE 24.471413\n",
      "sum_mpape:  23.68597813049952\n",
      "Epoch: 1   MAPE 24.318329\n",
      "sum_mpape:  23.695165864626567\n",
      "Epoch: 2   MAPE 24.3778\n",
      "sum_mpape:  23.688615127404532\n",
      "Epoch: 3   MAPE 24.329456\n",
      "sum_mpape:  24.166069356600442\n",
      "Epoch: 4   MAPE 25.134335\n",
      "sum_mpape:  23.760733763376873\n",
      "Epoch: 5   MAPE 24.404121\n",
      "sum_mpape:  24.42713010708491\n",
      "Epoch: 6   MAPE 24.861294\n",
      "sum_mpape:  24.252546771367392\n",
      "Epoch: 7   MAPE 24.462543\n",
      "sum_mpape:  24.17449346780777\n",
      "Epoch: 8   MAPE 24.344397\n",
      "sum_mpape:  24.179450885454813\n",
      "Epoch: 9   MAPE 24.632973\n",
      "sum_mpape:  23.921228619416556\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    min_mape = 50\n",
    "    sum_mpape = 0\n",
    "    for i in range(batch_number):\n",
    "        input1,input2,real_label = get_batch_data(batch_size,i)\n",
    "        #print(input1,input2)\n",
    "        feed_dict={graph['x1']:input1,graph['x2']:input2,graph['Output_Seq']:real_label}\n",
    "        _,Pred_Context_out,Pred_Result,loss2,lossFC,ADJ_matrix,Output_seq,loss_shape = sess.run([graph['train'],graph['Pred_Context_out'],graph['Pred_Result'],graph['loss2'],graph['loss_FC'],graph['ADJ_matrix'],graph['Output_Seq'],graph['loss_shape']],feed_dict)\n",
    "        sum_mpape = sum_mpape + loss2\n",
    "        #_,loss1,Pred_Context_out,Pred_Result,Output_Seq,context_embed, loss2,lossFC,ADJ_matrix= sess.run([graph['train'],graph['loss1'],graph['Pred_Context_out'],graph['Pred_Result'],graph['Output_Seq'],graph['context_embed'],graph['loss2'],graph['loss_FC'],graph['ADJ_matrix']],feed_dict)\n",
    "        if(i%240==0):\n",
    "            #print(  \"MSE:\",loss1,\"  MAPE\",loss2,\"预测结果:\",Pred_Result[0],\" 目标: \",real_label[:,0,:])\n",
    "            #print( \"Epoch:\", epoch,  \"  MAPE\",loss2,\"Conxtloss\", lossFC,\"loss_shape:\", loss_shape,\" Pred_result:\", Pred_Result[0][:,15],Output_seq[0][:,15])\n",
    "            print( \"Epoch:\", epoch,  \"  MAPE\",loss2)\n",
    "    sum_mpape = sum_mpape/batch_number\n",
    "    print(\"sum_mpape: \",sum_mpape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 30684.371   MAPE 24.571455  FC_loss_mape: 37.08713\n",
      "预测结果: [63.109028 63.927647 83.38756  36.130096 70.47493 ] \n",
      " 目标:  [ 75.  79. 172.  74. 146.]\n",
      "MSE: 176769.02   MAPE 29.89264  FC_loss_mape: 42.462154\n",
      "预测结果: [ 89.402214  96.981606 118.19715   60.886074 100.18897 ] \n",
      " 目标:  [143. 117. 274. 118. 267.]\n",
      "MSE: 104953.0   MAPE 27.184048  FC_loss_mape: 40.771107\n",
      "预测结果: [104.51459   98.57275  128.17105   57.172256 112.6937  ] \n",
      " 目标:  [144. 107. 268. 113. 241.]\n",
      "MSE: 7481.6494   MAPE 27.051487  FC_loss_mape: 29.35692\n",
      "预测结果: [ 0.          0.99654114 59.46162    48.289196   48.82595   ] \n",
      " 目标:  [ 48.  49. 119.  47.  73.]\n",
      "MSE: 343914.53   MAPE 24.107117  FC_loss_mape: 42.082794\n",
      "预测结果: [113.353546 104.58995  137.08067   59.081673 123.09593 ] \n",
      " 目标:  [128. 113. 117.  49.  72.]\n",
      "MSE: 186358.34   MAPE 28.918982  FC_loss_mape: 42.680553\n",
      "预测结果: [117.26412  105.21964  133.8081    60.400738 121.99939 ] \n",
      " 目标:  [187. 151. 143.  68. 122.]\n",
      "MSE: 162110.66   MAPE 25.643156  FC_loss_mape: 42.417477\n",
      "预测结果: [114.544136 107.73883  139.5701    62.304497 123.30977 ] \n",
      " 目标:  [183. 161. 128.  71. 120.]\n",
      "MSE: 62050.07   MAPE 23.879278  FC_loss_mape: 38.41074\n",
      "预测结果: [ 98.3128    94.55547  120.03673   51.824806 101.75795 ] \n",
      " 目标:  [ 86.  86. 158.  99. 118.]\n",
      "MSE: 534850.3   MAPE 26.7841  FC_loss_mape: 44.78374\n",
      "预测结果: [178.18407 176.0395  220.04762 104.70097 170.8555 ] \n",
      " 目标:  [132. 112. 253. 160. 182.]\n",
      "MSE: 544139.5   MAPE 27.323807  FC_loss_mape: 43.809544\n",
      "预测结果: [142.01001  143.48238  180.97072   80.642265 145.19768 ] \n",
      " 目标:  [119. 117. 251. 187. 190.]\n"
     ]
    }
   ],
   "source": [
    "#保存最后一次训练的数据，可以用作评估(不要改动batch_size),都存为240*108*6\n",
    "#存取FC的预测 \n",
    "output_data1 = np.ones((batch_number*batch_size,108,6))\n",
    "#存取GNN的预测\n",
    "output_data2 = np.ones((batch_number*batch_size,108,6))\n",
    "#存取对应的真实值\n",
    "real_data = np.ones((batch_number*batch_size,108,6))\n",
    "\n",
    "for i in range(batch_number):\n",
    "        input1,input2,real_label = get_batch_data(batch_size,i)\n",
    "        \n",
    "        #训练并得到相应的预测和loss\n",
    "        feed_dict={graph['x1']:input1,graph['x2']:input2,graph['Output_Seq']:real_label}\n",
    "        _,loss1,loss2,loss_FC,Pred_Result,Pred_Context_out,context_embed,ADJ_matrix = sess.run([graph['train'],graph['loss1'],graph['loss2'],graph['loss_FC'],graph['Pred_Result'],graph['Pred_Context_out'],graph['context_embed'],graph['ADJ_matrix']],feed_dict)\n",
    "        \n",
    "        #将每一个batch的数据存起来\n",
    "        output_data1[i,:,:] = Pred_Context_out.transpose(0,2,1)[0]\n",
    "        real_data[i] = real_label.transpose(0,2,1)[0]\n",
    "        for j in range(6):\n",
    "            output_data2[i,:,j] = Pred_Result[0][j,:]\n",
    "        \n",
    "        if(i%24==0):\n",
    "            #print(  \"MSE:\",loss1,\"  MAPE\",loss2,\"预测结果:\",Pred_Result[0],\" 目标: \",real_label[:,0,:])\n",
    "            print(  \"MSE:\",loss1,\"  MAPE\",loss2, \" FC_loss_mape:\",loss_FC)\n",
    "            print(\"预测结果:\",Pred_Result[0][0,15:20],\"\\n 目标: \",real_label[0,0,15:20])\n",
    "np.save('data/Context_output.npy',output_data1)\n",
    "np.save('data/GNN_output.npy',output_data2)\n",
    "np.save('data/GNN_realdata.npy',real_data)\n",
    "np.save('data/GNN_context_embed.npy',context_embed.squeeze(axis=0))\n",
    "np.save('data/GNN_context_matrix.npy',ADJ_matrix.squeeze(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Batch: 0 TestSample: 0---18   \n",
      "MAPE 24.339611 Conxtloss 40.45152\n",
      "Pred_result: [ 63.137836 126.57937  228.54805  313.63037  374.9515   435.60422 ] \n",
      "Real_label [ 75. 102. 180. 300. 396. 525.]\n",
      "\n",
      "\n",
      "Batch: 2 TestSample: 36---54   \n",
      "MAPE 30.992336 Conxtloss 40.45152\n",
      "Pred_result: [145.89302  135.31557   95.15185   67.99739   58.353992  46.778698] \n",
      "Real_label [125. 116.  90.  82.  63.  42.]\n",
      "\n",
      "\n",
      "Batch: 4 TestSample: 72---90   \n",
      "MAPE 19.791958 Conxtloss 40.45152\n",
      "Pred_result: [422.9172  679.8204  544.4015  586.4971  550.45654 550.23694] \n",
      "Real_label [692. 687. 615. 582. 560. 510.]\n",
      "\n",
      "\n",
      "Batch: 6 TestSample: 108---126   \n",
      "MAPE 16.676014 Conxtloss 40.45152\n",
      "Pred_result: [603.7755  612.5392  584.1598  602.8427  619.202   614.47736] \n",
      "Real_label [554. 594. 512. 524. 515. 617.]\n",
      "\n",
      "\n",
      "Batch: 8 TestSample: 144---162   \n",
      "MAPE 28.44793 Conxtloss 40.45152\n",
      "Pred_result: [ 92.26492 212.07832 476.79993 765.2735  646.8296  565.88275] \n",
      "Real_label [125. 376. 800. 540. 477. 598.]\n",
      "\n",
      "\n",
      "Batch: 10 TestSample: 180---198   \n",
      "MAPE 30.29424 Conxtloss 40.45152\n",
      "Pred_result: [134.4848   118.83128   96.63046   75.132706  59.541836  57.849968] \n",
      "Real_label [109. 113. 110.  74.  55.  42.]\n",
      "\n",
      "\n",
      "Batch: 12 TestSample: 216---234   \n",
      "MAPE 19.735645 Conxtloss 40.45152\n",
      "Pred_result: [729.34576 907.15173 711.5148  586.39703 494.63678 542.92017] \n",
      "Real_label [1062.  853.  672.  603.  570.  460.]\n",
      "\n",
      "\n",
      "Batch: 14 TestSample: 252---270   \n",
      "MAPE 18.607414 Conxtloss 40.45152\n",
      "Pred_result: [591.27386 619.7027  616.0273  614.4444  646.8161  637.13153] \n",
      "Real_label [614. 621. 601. 548. 513. 556.]\n",
      "\n",
      "\n",
      "Batch: 16 TestSample: 288---306   \n",
      "MAPE 24.80013 Conxtloss 40.45152\n",
      "Pred_result: [ 82.49968 175.88486 328.6875  544.96045 636.06586 623.99774] \n",
      "Real_label [ 97. 175. 274. 437. 523. 704.]\n",
      "\n",
      "\n",
      "Batch: 18 TestSample: 324---342   \n",
      "MAPE 25.234554 Conxtloss 40.45152\n",
      "Pred_result: [132.05377 145.62943 107.02437  95.73677  80.62868  65.88873] \n",
      "Real_label [158. 123. 106.  96.  77.  56.]\n",
      "\n",
      "\n",
      "Batch: 20 TestSample: 360---378   \n",
      "MAPE 19.128687 Conxtloss 40.45152\n",
      "Pred_result: [518.37195 613.47485 523.1859  532.664   499.40063 500.80066] \n",
      "Real_label [682. 636. 568. 571. 548. 431.]\n",
      "\n",
      "\n",
      "Batch: 22 TestSample: 396---414   \n",
      "MAPE 17.526371 Conxtloss 40.45152\n",
      "Pred_result: [576.1068  519.57965 512.1343  533.8688  572.8669  572.24133] \n",
      "Real_label [572. 558. 529. 522. 540. 532.]\n",
      "\n",
      "\n",
      "Batch: 24 TestSample: 432---450   \n",
      "MAPE 29.982895 Conxtloss 40.45152\n",
      "Pred_result: [ 97.29104 209.37123 484.96457 760.05963 644.5145  579.34985] \n",
      "Real_label [143.      359.      854.      635.      407.26337 432.31604]\n",
      "\n",
      "\n",
      "Batch: 26 TestSample: 468---486   \n",
      "MAPE 28.344055 Conxtloss 40.45152\n",
      "Pred_result: [117.80378  119.41595   95.70581   84.67245   68.57898   61.403206] \n",
      "Real_label [147. 123. 103.  91.  59.  56.]\n",
      "\n",
      "\n",
      "Batch: 28 TestSample: 504---522   \n",
      "MAPE 19.3941 Conxtloss 40.45152\n",
      "Pred_result: [660.7584  955.6506  740.16064 657.3074  557.4098  593.65753] \n",
      "Real_label [1093.  857.  731.  636.  582.  531.]\n",
      "\n",
      "\n",
      "Batch: 30 TestSample: 540---558   \n",
      "MAPE 16.592665 Conxtloss 40.45152\n",
      "Pred_result: [597.0136  694.92596 680.72797 735.9461  783.96954 801.9337 ] \n",
      "Real_label [711. 690. 692. 740. 683. 705.]\n",
      "\n",
      "\n",
      "Batch: 32 TestSample: 576---594   \n",
      "MAPE 49.043243 Conxtloss 40.45152\n",
      "Pred_result: [339.15973 304.8198  425.26077 560.5664  427.98608 331.61346] \n",
      "Real_label [129.      338.      654.      545.10535 432.31604 361.26337]\n",
      "\n",
      "\n",
      "Batch: 34 TestSample: 612---630   \n",
      "MAPE 46.083 Conxtloss 40.45152\n",
      "Pred_result: [204.727   174.69032 148.58844 114.70639  82.56184  70.03764] \n",
      "Real_label [189. 167. 138. 119.  98.  65.]\n",
      "\n",
      "\n",
      "Batch: 36 TestSample: 648---666   \n",
      "MAPE 18.087032 Conxtloss 40.45152\n",
      "Pred_result: [475.87775 400.69534 400.25635 407.08466 385.12015 399.16113] \n",
      "Real_label [833. 685. 631. 613. 554. 529.]\n",
      "\n",
      "\n",
      "Batch: 38 TestSample: 684---702   \n",
      "MAPE 30.470612 Conxtloss 40.45152\n",
      "Pred_result: [483.67114 641.80786 632.59467 645.9384  689.4498  666.29565] \n",
      "Real_label [ 943. 1027.  983.  891.  909.  890.]\n",
      "\n",
      "\n",
      "Batch: 40 TestSample: 720---738   \n",
      "MAPE 49.990185 Conxtloss 40.45152\n",
      "Pred_result: [301.91577 210.31285 440.21283 761.1801  737.34937 540.63934] \n",
      "Real_label [ 143.  308.  806. 1189. 1175. 1039.]\n",
      "\n",
      "\n",
      "Batch: 42 TestSample: 756---774   \n",
      "MAPE 31.687887 Conxtloss 40.45152\n",
      "Pred_result: [163.42964 126.18549 109.09002  92.10708  78.62299  63.0659 ] \n",
      "Real_label [146. 106. 105.  86.  66.  53.]\n",
      "\n",
      "\n",
      "Batch: 44 TestSample: 792---810   \n",
      "MAPE 23.568542 Conxtloss 40.45152\n",
      "Pred_result: [413.02664 603.46106 739.3     637.1252  556.41766 562.7452 ] \n",
      "Real_label [710. 869. 707. 610. 576. 606.]\n",
      "\n",
      "\n",
      "Batch: 46 TestSample: 828---846   \n",
      "MAPE 20.823868 Conxtloss 40.45152\n",
      "Pred_result: [490.47333 625.92834 650.32117 656.09033 638.09125 697.50006] \n",
      "Real_label [717. 787. 701. 705. 659. 645.]\n",
      "\n",
      "\n",
      "Batch: 48 TestSample: 864---882   \n",
      "MAPE 28.716602 Conxtloss 40.45152\n",
      "Pred_result: [113.432144 209.03598  401.22638  758.51447  671.2574   641.6951  ] \n",
      "Real_label [144. 260. 539. 478. 548. 672.]\n",
      "\n",
      "\n",
      "Batch: 50 TestSample: 900---918   \n",
      "MAPE 26.501371 Conxtloss 40.45152\n",
      "Pred_result: [112.80025  137.65247   98.704666  83.97924   65.16592   55.545586] \n",
      "Real_label [167. 161. 106.  90.  75.  57.]\n",
      "\n",
      "\n",
      "Batch: 52 TestSample: 936---954   \n",
      "MAPE 24.17545 Conxtloss 40.45152\n",
      "Pred_result: [463.98697 955.08154 707.7448  650.4936  593.38324 607.1624 ] \n",
      "Real_label [1012.  884.  727.  676.  592.  615.]\n",
      "\n",
      "\n",
      "Batch: 54 TestSample: 972---990   \n",
      "MAPE 21.244839 Conxtloss 40.45152\n",
      "Pred_result: [397.03903 641.41925 724.51624 650.681   576.85956 633.48285] \n",
      "Real_label [724. 812. 722. 631. 572. 667.]\n",
      "\n",
      "\n",
      "Batch: 56 TestSample: 1008---1026   \n",
      "MAPE 25.238918 Conxtloss 40.45152\n",
      "Pred_result: [104.69823 182.71591 342.13364 641.93695 709.1524  611.1908 ] \n",
      "Real_label [ 80. 223. 412. 506. 535. 647.]\n",
      "\n",
      "\n",
      "Batch: 58 TestSample: 1044---1062   \n",
      "MAPE 30.804379 Conxtloss 40.45152\n",
      "Pred_result: [ 86.28055  108.35379   83.689606  64.93395   48.548904  51.572887] \n",
      "Real_label [145.  87.  90.  70.  63.  38.]\n",
      "\n",
      "\n",
      "Batch: 60 TestSample: 1080---1098   \n",
      "MAPE 23.12638 Conxtloss 40.45152\n",
      "Pred_result: [503.31393 886.22955 623.9948  545.43536 487.13678 527.5861 ] \n",
      "Real_label [900. 740. 690. 593. 514. 505.]\n",
      "\n",
      "\n",
      "Batch: 62 TestSample: 1116---1134   \n",
      "MAPE 20.456196 Conxtloss 40.45152\n",
      "Pred_result: [471.74728 605.74774 633.2881  708.74756 744.46136 762.4635 ] \n",
      "Real_label [693. 679. 686. 732. 759. 697.]\n",
      "\n",
      "\n",
      "Batch: 64 TestSample: 1152---1170   \n",
      "MAPE 26.559662 Conxtloss 40.45152\n",
      "Pred_result: [ 99.35027 180.76196 324.28714 568.64764 798.3023  862.5444 ] \n",
      "Real_label [112. 206. 448. 619. 754. 795.]\n",
      "\n",
      "\n",
      "Batch: 66 TestSample: 1188---1206   \n",
      "MAPE 36.462765 Conxtloss 40.45152\n",
      "Pred_result: [60.62849  75.90146  68.71658  53.229393 45.805546 39.719604] \n",
      "Real_label [91. 67. 60. 57. 44. 29.]\n",
      "\n",
      "\n",
      "Batch: 68 TestSample: 1224---1242   \n",
      "MAPE 19.816181 Conxtloss 40.45152\n",
      "Pred_result: [343.08942 384.34552 325.7202  361.9969  337.12164 320.51913] \n",
      "Real_label [461. 421. 396. 456. 408. 362.]\n",
      "\n",
      "\n",
      "Batch: 70 TestSample: 1260---1278   \n",
      "MAPE 20.944077 Conxtloss 40.45152\n",
      "Pred_result: [341.9832  393.9992  390.55426 424.63513 452.57355 446.23282] \n",
      "Real_label [429. 413. 456. 474. 456. 428.]\n",
      "\n",
      "\n",
      "Batch: 72 TestSample: 1296---1314   \n",
      "MAPE 26.426105 Conxtloss 40.45152\n",
      "Pred_result: [  2.248697  70.31635   92.17966  129.1103   145.02853  166.59906 ] \n",
      "Real_label [ 48.  74.  99. 113. 178. 216.]\n",
      "\n",
      "\n",
      "Batch: 74 TestSample: 1332---1350   \n",
      "MAPE 62.793926 Conxtloss 40.45152\n",
      "Pred_result: [85.12725  25.707872 24.105453 26.50349  18.936245 28.73471 ] \n",
      "Real_label [43. 42. 21. 23. 25. 20.]\n",
      "\n",
      "\n",
      "Batch: 76 TestSample: 1368---1386   \n",
      "MAPE 20.587103 Conxtloss 40.45152\n",
      "Pred_result: [212.87764 294.37277 288.66046 302.5896  274.30792 237.99176] \n",
      "Real_label [371. 350. 321. 299. 287. 251.]\n",
      "\n",
      "\n",
      "Batch: 78 TestSample: 1404---1422   \n",
      "MAPE 23.012295 Conxtloss 40.45152\n",
      "Pred_result: [349.18906 430.5291  412.07672 479.2958  533.2971  532.3658 ] \n",
      "Real_label [568. 544. 636. 644. 612. 596.]\n",
      "\n",
      "\n",
      "Batch: 80 TestSample: 1440---1458   \n",
      "MAPE 23.132671 Conxtloss 40.45152\n",
      "Pred_result: [ 38.32021  81.18887 120.09869 166.4188  191.4611  216.09598] \n",
      "Real_label [ 37.  49.  96. 160. 204. 287.]\n",
      "\n",
      "\n",
      "Batch: 82 TestSample: 1476---1494   \n",
      "MAPE 29.660439 Conxtloss 40.45152\n",
      "Pred_result: [53.93789  67.29907  56.88733  42.92682  41.706806 26.562872] \n",
      "Real_label [87.       57.       50.       49.       32.       24.109756]\n",
      "\n",
      "\n",
      "Batch: 84 TestSample: 1512---1530   \n",
      "MAPE 24.557402 Conxtloss 40.45152\n",
      "Pred_result: [241.43874 416.8008  395.41718 435.1985  384.21243 376.24692] \n",
      "Real_label [435. 470. 453. 458. 441. 449.]\n",
      "\n",
      "\n",
      "Batch: 86 TestSample: 1548---1566   \n",
      "MAPE 22.227139 Conxtloss 40.45152\n",
      "Pred_result: [310.94412 514.12933 657.5646  705.40967 726.26324 711.286  ] \n",
      "Real_label [562. 617. 717. 735. 742. 650.]\n",
      "\n",
      "\n",
      "Batch: 88 TestSample: 1584---1602   \n",
      "MAPE 24.74625 Conxtloss 40.45152\n",
      "Pred_result: [ 89.417076 178.11635  339.95462  597.88995  891.9708   935.68024 ] \n",
      "Real_label [118. 235. 465. 843. 924. 918.]\n",
      "\n",
      "\n",
      "Batch: 90 TestSample: 1620---1638   \n",
      "MAPE 25.066984 Conxtloss 40.45152\n",
      "Pred_result: [74.26313  71.52074  49.035248 42.05081  33.402805 29.24009 ] \n",
      "Real_label [124.  77.  53.  48.  42.  22.]\n",
      "\n",
      "\n",
      "Batch: 92 TestSample: 1656---1674   \n",
      "MAPE 21.693762 Conxtloss 40.45152\n",
      "Pred_result: [453.73392 830.1073  577.9065  591.8085  527.00104 484.82816] \n",
      "Real_label [1184.  836.  697.  617.  632.  618.]\n",
      "\n",
      "\n",
      "Batch: 94 TestSample: 1692---1710   \n",
      "MAPE 19.091143 Conxtloss 40.45152\n",
      "Pred_result: [296.64035 697.91095 791.9266  811.26917 888.0559  863.37067] \n",
      "Real_label [750. 820. 838. 880. 884. 974.]\n",
      "\n",
      "\n",
      "Batch: 96 TestSample: 1728---1746   \n",
      "MAPE 24.159964 Conxtloss 40.45152\n",
      "Pred_result: [ 101.179184  211.63512   448.80698   878.6007   1314.0315   1320.5941  ] \n",
      "Real_label [ 128.  304.  669. 1222. 1291. 1352.]\n",
      "\n",
      "\n",
      "Batch: 98 TestSample: 1764---1782   \n",
      "MAPE 22.756287 Conxtloss 40.45152\n",
      "Pred_result: [73.28177  90.68969  67.758995 46.09219  35.24265  31.650524] \n",
      "Real_label [124.  96.  61.  48.  41.  40.]\n",
      "\n",
      "\n",
      "Batch: 100 TestSample: 1800---1818   \n",
      "MAPE 19.74979 Conxtloss 40.45152\n",
      "Pred_result: [ 550.13403 1147.318    923.3875   762.95     644.6721   628.4887 ] \n",
      "Real_label [1525. 1140.  924.  783.  791.  736.]\n",
      "\n",
      "\n",
      "Batch: 102 TestSample: 1836---1854   \n",
      "MAPE 17.948608 Conxtloss 40.45152\n",
      "Pred_result: [400.41293 698.0076  734.37036 753.0934  761.7619  760.6664 ] \n",
      "Real_label [661. 775. 759. 774. 760. 710.]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch: 104 TestSample: 1872---1890   \n",
      "MAPE 23.471067 Conxtloss 40.45152\n",
      "Pred_result: [ 107.32301  213.5696   433.843    872.6355  1256.6056  1395.7965 ] \n",
      "Real_label [ 160.  294.  670. 1293. 1312. 1357.]\n",
      "\n",
      "\n",
      "Batch: 106 TestSample: 1908---1926   \n",
      "MAPE 22.008507 Conxtloss 40.45152\n",
      "Pred_result: [ 84.022766 108.928444  72.71252   58.476185  47.3502    37.610706] \n",
      "Real_label [180.  98.  83.  49.  45.  29.]\n",
      "\n",
      "\n",
      "Batch: 108 TestSample: 1944---1962   \n",
      "MAPE 20.885357 Conxtloss 40.45152\n",
      "Pred_result: [ 452.1393  1269.6588  1028.5056   878.68964  739.25     714.23486] \n",
      "Real_label [1572. 1386. 1145.  895.  910.  956.]\n",
      "\n",
      "\n",
      "Batch: 110 TestSample: 1980---1998   \n",
      "MAPE 20.871439 Conxtloss 40.45152\n",
      "Pred_result: [ 405.17682  838.91046  940.4353   968.0232  1010.26825 1038.8882 ] \n",
      "Real_label [1026. 1018. 1124. 1184. 1201. 1188.]\n",
      "\n",
      "\n",
      "Batch: 112 TestSample: 2016---2034   \n",
      "MAPE 27.886358 Conxtloss 40.45152\n",
      "Pred_result: [ 92.49066 154.56633 278.00366 473.71243 673.41095 761.70966] \n",
      "Real_label [106. 188. 400. 658. 886. 961.]\n",
      "\n",
      "\n",
      "Batch: 114 TestSample: 2052---2070   \n",
      "MAPE 27.707062 Conxtloss 40.45152\n",
      "Pred_result: [ 55.166622 113.32037   76.376915  52.56008   41.157917  33.77796 ] \n",
      "Real_label [135.  92.  68.  41.  30.  38.]\n",
      "\n",
      "\n",
      "Batch: 116 TestSample: 2088---2106   \n",
      "MAPE 21.550344 Conxtloss 40.45152\n",
      "Pred_result: [ 551.7946  1157.8977   898.95105  734.3228   654.8131   606.7226 ] \n",
      "Real_label [1535. 1247.  986.  897.  899.  806.]\n",
      "\n",
      "\n",
      "Batch: 118 TestSample: 2124---2142   \n",
      "MAPE 108.04387 Conxtloss 40.45152\n",
      "Pred_result: [492.1767  731.7021  776.7556  737.3752  855.86115 740.8341 ] \n",
      "Real_label [749. 695. 726. 840. 866.  89.]\n",
      "\n",
      "\n",
      "Batch: 120 TestSample: 2160---2178   \n",
      "MAPE 28.449936 Conxtloss 40.45152\n",
      "Pred_result: [121.676445 271.74646  619.3076   775.23816  636.98645  601.9648  ] \n",
      "Real_label [187. 660. 938. 871. 778. 853.]\n",
      "\n",
      "\n",
      "Batch: 122 TestSample: 2196---2214   \n",
      "MAPE 26.203505 Conxtloss 40.45152\n",
      "Pred_result: [102.88502  115.28003   78.646736  70.031166  52.56337   41.442673] \n",
      "Real_label [153. 134. 105.  77.  39.  52.]\n",
      "\n",
      "\n",
      "Batch: 124 TestSample: 2232---2250   \n",
      "MAPE 19.687855 Conxtloss 40.45152\n",
      "Pred_result: [ 591.7322  1179.9298   914.53485  768.3609   683.8629   693.88947] \n",
      "Real_label [1564. 1229.  912.  850.  841.  923.]\n",
      "\n",
      "\n",
      "Batch: 126 TestSample: 2268---2286   \n",
      "MAPE 17.818897 Conxtloss 40.45152\n",
      "Pred_result: [444.34293 820.09436 830.74493 884.1614  937.6087  915.58923] \n",
      "Real_label [724. 744. 795. 873. 849. 798.]\n",
      "\n",
      "\n",
      "Batch: 128 TestSample: 2304---2322   \n",
      "MAPE 21.038464 Conxtloss 40.45152\n",
      "Pred_result: [ 94.00015 207.12685 391.44394 542.7944  753.3092  898.1429 ] \n",
      "Real_label [ 155.  342.  443.  712.  931. 1122.]\n",
      "\n",
      "\n",
      "Batch: 130 TestSample: 2340---2358   \n",
      "MAPE 25.130814 Conxtloss 40.45152\n",
      "Pred_result: [ 87.51337  150.18124  102.543434  87.081505  71.17797   58.036053] \n",
      "Real_label [178. 151. 116. 110.  75.  55.]\n",
      "\n",
      "\n",
      "Batch: 132 TestSample: 2376---2394   \n",
      "MAPE 17.139568 Conxtloss 40.45152\n",
      "Pred_result: [465.46512 894.43274 811.37006 748.6682  722.1438  647.0095 ] \n",
      "Real_label [1072.  947.  854.  868.  886.  718.]\n",
      "\n",
      "\n",
      "Batch: 134 TestSample: 2412---2430   \n",
      "MAPE 16.227627 Conxtloss 40.45152\n",
      "Pred_result: [436.84763 769.31165 823.90894 876.03815 899.1443  867.78156] \n",
      "Real_label [701. 726. 786. 912. 849. 823.]\n",
      "\n",
      "\n",
      "Batch: 136 TestSample: 2448---2466   \n",
      "MAPE 25.489841 Conxtloss 40.45152\n",
      "Pred_result: [ 148.77002  252.55635  649.7941  1193.9729  1560.2656  1511.7429 ] \n",
      "Real_label [ 194.  664. 1224. 1608. 1595. 1412.]\n",
      "\n",
      "\n",
      "Batch: 138 TestSample: 2484---2502   \n",
      "MAPE 26.812492 Conxtloss 40.45152\n",
      "Pred_result: [ 94.84409  138.76367   83.29341   71.55651   59.23885   41.939472] \n",
      "Real_label [163. 106.  89.  76.  76.  38.]\n",
      "\n",
      "\n",
      "Batch: 140 TestSample: 2520---2538   \n",
      "MAPE 27.79682 Conxtloss 40.45152\n",
      "Pred_result: [ 456.85162  808.6094  1083.0859   886.9157   745.5856   696.7182 ] \n",
      "Real_label [ 996. 1127.  917.  781.  803.  839.]\n",
      "\n",
      "\n",
      "Batch: 142 TestSample: 2556---2574   \n",
      "MAPE 17.783688 Conxtloss 40.45152\n",
      "Pred_result: [498.60773 779.83795 832.02606 790.1216  811.7472  829.90125] \n",
      "Real_label [685. 726. 711. 665. 714. 720.]\n",
      "\n",
      "\n",
      "Batch: 144 TestSample: 2592---2610   \n",
      "MAPE 24.979118 Conxtloss 40.45152\n",
      "Pred_result: [117.1128  271.25916 700.8894  940.5438  813.66656 825.0619 ] \n",
      "Real_label [183. 634. 878. 859. 779. 822.]\n",
      "\n",
      "\n",
      "Batch: 146 TestSample: 2628---2646   \n",
      "MAPE 30.425545 Conxtloss 40.45152\n",
      "Pred_result: [123.31533 195.55315 151.21408 105.22207  91.54383  67.44865] \n",
      "Real_label [236. 199. 137. 114.  79.  65.]\n",
      "\n",
      "\n",
      "Batch: 148 TestSample: 2664---2682   \n",
      "MAPE 20.911999 Conxtloss 40.45152\n",
      "Pred_result: [365.11252 994.9122  919.55316 823.4977  765.9362  754.92126] \n",
      "Real_label [1041. 1013.  861.  853.  779.  819.]\n",
      "\n",
      "\n",
      "Batch: 150 TestSample: 2700---2718   \n",
      "MAPE 18.515512 Conxtloss 40.45152\n",
      "Pred_result: [393.54196 840.3183  904.163   881.1864  900.8756  888.4001 ] \n",
      "Real_label [948. 863. 924. 867. 899. 896.]\n",
      "\n",
      "\n",
      "Batch: 152 TestSample: 2736---2754   \n",
      "MAPE 21.567236 Conxtloss 40.45152\n",
      "Pred_result: [150.67038 327.12302 780.5706  887.0773  781.7961  737.5322 ] \n",
      "Real_label [178. 665. 913. 777. 776. 760.]\n",
      "\n",
      "\n",
      "Batch: 154 TestSample: 2772---2790   \n",
      "MAPE 30.010063 Conxtloss 40.45152\n",
      "Pred_result: [ 85.62672  129.43082   98.480545  81.41668   63.89554   49.336914] \n",
      "Real_label [156.  93.  99.  68.  43.  39.]\n",
      "\n",
      "\n",
      "Batch: 156 TestSample: 2808---2826   \n",
      "MAPE 24.04891 Conxtloss 40.45152\n",
      "Pred_result: [312.7452  739.82587 782.5269  886.371   743.9975  752.3802 ] \n",
      "Real_label [826. 932. 883. 829. 842. 815.]\n",
      "\n",
      "\n",
      "Batch: 158 TestSample: 2844---2862   \n",
      "MAPE 17.497242 Conxtloss 40.45152\n",
      "Pred_result: [484.9866  412.59317 456.5554  478.8032  450.91022 453.90244] \n",
      "Real_label [540. 590. 559. 590. 485. 501.]\n",
      "\n",
      "\n",
      "Batch: 160 TestSample: 2880---2898   \n",
      "MAPE 29.36907 Conxtloss 40.45152\n",
      "Pred_result: [117.45182 180.52567 364.9347  636.5352  672.99945 591.4902 ] \n",
      "Real_label [118. 202. 363. 576. 824. 634.]\n",
      "\n",
      "\n",
      "Batch: 162 TestSample: 2916---2934   \n",
      "MAPE 24.37894 Conxtloss 40.45152\n",
      "Pred_result: [121.355774  59.853405  57.76181   48.254627  48.672314  49.158077] \n",
      "Real_label [87. 60. 63. 51. 46. 37.]\n",
      "\n",
      "\n",
      "Batch: 164 TestSample: 2952---2970   \n",
      "MAPE 19.647518 Conxtloss 40.45152\n",
      "Pred_result: [649.868   391.55045 356.21353 347.99777 289.3007  268.38678] \n",
      "Real_label [502. 455. 390. 351. 320. 239.]\n",
      "\n",
      "\n",
      "Batch: 166 TestSample: 2988---3006   \n",
      "MAPE 20.058434 Conxtloss 40.45152\n",
      "Pred_result: [432.7269  446.98334 479.16983 511.5403  562.5724  537.02203] \n",
      "Real_label [475. 452. 514. 506. 548. 559.]\n",
      "\n",
      "\n",
      "Batch: 168 TestSample: 3024---3042   \n",
      "MAPE 23.854849 Conxtloss 40.45152\n",
      "Pred_result: [ 92.44768 146.46071 208.61148 244.01564 353.2722  439.52155] \n",
      "Real_label [ 86. 119. 156. 262. 317. 324.]\n",
      "\n",
      "\n",
      "Batch: 170 TestSample: 3060---3078   \n",
      "MAPE 24.89729 Conxtloss 40.45152\n",
      "Pred_result: [57.295876 51.574482 35.392044 33.495396 30.910192 31.80063 ] \n",
      "Real_label [61. 39. 39. 31. 23. 27.]\n",
      "\n",
      "\n",
      "Batch: 172 TestSample: 3096---3114   \n",
      "MAPE 17.302702 Conxtloss 40.45152\n",
      "Pred_result: [686.1861  358.76114 372.95193 313.17834 298.58966 275.8039 ] \n",
      "Real_label [401. 363. 280. 297. 285. 224.]\n",
      "\n",
      "\n",
      "Batch: 174 TestSample: 3132---3150   \n",
      "MAPE 16.91624 Conxtloss 40.45152\n",
      "Pred_result: [469.98203 422.0953  431.39264 526.6432  558.0392  578.715  ] \n",
      "Real_label [552. 629. 647. 616. 709. 637.]\n",
      "\n",
      "\n",
      "Batch: 176 TestSample: 3168---3186   \n",
      "MAPE 27.843338 Conxtloss 40.45152\n",
      "Pred_result: [133.49321 184.31615 355.5733  680.85126 943.60077 955.6407 ] \n",
      "Real_label [ 128.  204.  404.  913. 1311. 1185.]\n",
      "\n",
      "\n",
      "Batch: 178 TestSample: 3204---3222   \n",
      "MAPE 22.391354 Conxtloss 40.45152\n",
      "Pred_result: [87.85416  58.37946  45.326633 41.579346 39.15914  43.186497] \n",
      "Real_label [65. 67. 49. 35. 35. 50.]\n",
      "\n",
      "\n",
      "Batch: 180 TestSample: 3240---3258   \n",
      "MAPE 16.701447 Conxtloss 40.45152\n",
      "Pred_result: [683.5268  439.40085 348.3075  320.71072 325.15405 302.0615 ] \n",
      "Real_label [402. 323. 281. 299. 287. 221.]\n",
      "\n",
      "\n",
      "Batch: 182 TestSample: 3276---3294   \n",
      "MAPE 15.466247 Conxtloss 40.45152\n",
      "Pred_result: [474.86386 515.3627  539.38745 601.9827  621.68304 612.07446] \n",
      "Real_label [597. 641. 656. 744. 720. 707.]\n",
      "\n",
      "\n",
      "Batch: 184 TestSample: 3312---3330   \n",
      "MAPE 23.17863 Conxtloss 40.45152\n",
      "Pred_result: [103.92155 182.54118 259.13934 359.30667 495.0312  586.58527] \n",
      "Real_label [100. 147. 224. 492. 600. 469.]\n",
      "\n",
      "\n",
      "Batch: 186 TestSample: 3348---3366   \n",
      "MAPE 24.168127 Conxtloss 40.45152\n",
      "Pred_result: [96.43471  66.65952  53.654045 46.152027 47.502316 41.58163 ] \n",
      "Real_label [72. 60. 51. 29. 31. 27.]\n",
      "\n",
      "\n",
      "Batch: 188 TestSample: 3384---3402   \n",
      "MAPE 18.503069 Conxtloss 40.45152\n",
      "Pred_result: [460.9308  284.0931  268.39038 265.9079  250.49646 243.99947] \n",
      "Real_label [254. 211. 211. 233. 247. 188.]\n",
      "\n",
      "\n",
      "Batch: 190 TestSample: 3420---3438   \n",
      "MAPE 15.745929 Conxtloss 40.45152\n",
      "Pred_result: [443.94785 456.924   469.83112 500.51398 508.07483 504.97864] \n",
      "Real_label [470. 585. 613. 582. 574. 578.]\n",
      "\n",
      "\n",
      "Batch: 192 TestSample: 3456---3474   \n",
      "MAPE 26.354574 Conxtloss 40.45152\n",
      "Pred_result: [174.34714 229.84601 399.4173  660.5448  947.0557  925.6629 ] \n",
      "Real_label [ 132.  224.  360.  903. 1283. 1271.]\n",
      "\n",
      "\n",
      "Batch: 194 TestSample: 3492---3510   \n",
      "MAPE 22.989813 Conxtloss 40.45152\n",
      "Pred_result: [88.54571  49.842953 45.46206  44.213566 34.457855 38.547337] \n",
      "Real_label [70. 84. 54. 38. 41. 33.]\n",
      "\n",
      "\n",
      "Batch: 196 TestSample: 3528---3546   \n",
      "MAPE 17.90116 Conxtloss 40.45152\n",
      "Pred_result: [750.12115 446.39957 359.0992  289.4483  286.1185  260.5597 ] \n",
      "Real_label [463. 365. 278. 307. 323. 263.]\n",
      "\n",
      "\n",
      "Batch: 198 TestSample: 3564---3582   \n",
      "MAPE 18.19059 Conxtloss 40.45152\n",
      "Pred_result: [491.54285 509.26962 534.6498  602.8919  643.77637 602.779  ] \n",
      "Real_label [547. 568. 632. 680. 675. 630.]\n",
      "\n",
      "\n",
      "Batch: 200 TestSample: 3600---3618   \n",
      "MAPE 25.760693 Conxtloss 40.45152\n",
      "Pred_result: [153.56172 230.91362 366.00287 689.42303 954.68054 988.5166 ] \n",
      "Real_label [ 136.  205.  367.  965. 1318. 1185.]\n",
      "\n",
      "\n",
      "Batch: 202 TestSample: 3636---3654   \n",
      "MAPE 24.69159 Conxtloss 40.45152\n",
      "Pred_result: [96.22565  60.876484 55.870216 44.163    46.729027 36.182064] \n",
      "Real_label [108.  87.  61.  54.  55.  34.]\n",
      "\n",
      "\n",
      "Batch: 204 TestSample: 3672---3690   \n",
      "MAPE 17.90802 Conxtloss 40.45152\n",
      "Pred_result: [549.92725 364.2299  265.40854 263.3584  247.48819 252.63376] \n",
      "Real_label [341. 294. 254. 262. 284. 215.]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 206 TestSample: 3708---3726   \n",
      "MAPE 18.948143 Conxtloss 40.45152\n",
      "Pred_result: [498.1918  454.51886 443.69724 485.76245 451.93848 450.5404 ] \n",
      "Real_label [346. 321. 406. 434. 427. 478.]\n",
      "\n",
      "\n",
      "Batch: 208 TestSample: 3744---3762   \n",
      "MAPE 26.44957 Conxtloss 40.45152\n",
      "Pred_result: [ 153.2942   225.52438  417.82037  726.73376 1011.4833  1007.431  ] \n",
      "Real_label [ 141.  205.  361. 1017. 1268. 1282.]\n",
      "\n",
      "\n",
      "Batch: 210 TestSample: 3780---3798   \n",
      "MAPE 25.227974 Conxtloss 40.45152\n",
      "Pred_result: [88.4314   58.9283   45.634125 46.6516   40.566895 38.429356] \n",
      "Real_label [69. 61. 59. 54. 46. 37.]\n",
      "\n",
      "\n",
      "Batch: 212 TestSample: 3816---3834   \n",
      "MAPE 19.162888 Conxtloss 40.45152\n",
      "Pred_result: [584.6714  423.48566 319.73993 288.69086 252.05455 244.47057] \n",
      "Real_label [407. 332. 293. 322. 299. 249.]\n",
      "\n",
      "\n",
      "Batch: 214 TestSample: 3852---3870   \n",
      "MAPE 13.895942 Conxtloss 40.45152\n",
      "Pred_result: [503.55142 444.8172  488.3889  510.9224  551.86035 510.14438] \n",
      "Real_label [491. 558. 607. 632. 573. 622.]\n",
      "\n",
      "\n",
      "Batch: 216 TestSample: 3888---3906   \n",
      "MAPE 27.309185 Conxtloss 40.45152\n",
      "Pred_result: [ 134.1512   210.04732  348.19122  662.51733  957.08704 1030.8329 ] \n",
      "Real_label [ 119.  213.  367.  856. 1320. 1217.]\n",
      "\n",
      "\n",
      "Batch: 218 TestSample: 3924---3942   \n",
      "MAPE 25.492203 Conxtloss 40.45152\n",
      "Pred_result: [105.93147   62.43349   54.55462   40.64961   40.379337  42.984783] \n",
      "Real_label [70. 79. 49. 50. 41. 42.]\n",
      "\n",
      "\n",
      "Batch: 220 TestSample: 3960---3978   \n",
      "MAPE 18.899284 Conxtloss 40.45152\n",
      "Pred_result: [742.5005  584.7501  479.13864 370.43063 357.60944 358.43002] \n",
      "Real_label [548. 403. 374. 320. 396. 256.]\n",
      "\n",
      "\n",
      "Batch: 222 TestSample: 3996---4014   \n",
      "MAPE 21.729979 Conxtloss 40.45152\n",
      "Pred_result: [488.93185 436.06866 462.33923 488.71445 504.5918  492.23383] \n",
      "Real_label [410. 461. 512. 539. 548. 538.]\n",
      "\n",
      "\n",
      "Batch: 224 TestSample: 4032---4050   \n",
      "MAPE 25.30561 Conxtloss 40.45152\n",
      "Pred_result: [116.17506 168.88033 233.94116 308.42557 391.8188  478.98575] \n",
      "Real_label [109. 113. 181. 280. 336. 312.]\n",
      "\n",
      "\n",
      "Batch: 226 TestSample: 4068---4086   \n",
      "MAPE 23.893433 Conxtloss 40.45152\n",
      "Pred_result: [84.85668  43.392403 36.92387  35.42161  36.228794 28.993525] \n",
      "Real_label [66. 47. 43. 45. 31. 27.]\n",
      "\n",
      "\n",
      "Batch: 228 TestSample: 4104---4122   \n",
      "MAPE 21.287718 Conxtloss 40.45152\n",
      "Pred_result: [797.9964  481.3223  388.68686 382.7552  317.95044 318.27173] \n",
      "Real_label [408. 337. 326. 288. 266. 235.]\n",
      "\n",
      "\n",
      "Batch: 230 TestSample: 4140---4158   \n",
      "MAPE 14.850478 Conxtloss 40.45152\n",
      "Pred_result: [513.9511  447.89758 488.80804 524.33606 526.5015  477.64117] \n",
      "Real_label [497. 565. 650. 589. 587. 629.]\n",
      "\n",
      "\n",
      "Batch: 232 TestSample: 4176---4194   \n",
      "MAPE 25.249968 Conxtloss 40.45152\n",
      "Pred_result: [ 182.22064  252.67212  392.88293  738.03925 1072.191   1024.8289 ] \n",
      "Real_label [ 157.  201.  404.  930. 1326. 1237.]\n",
      "\n",
      "\n",
      "Batch: 234 TestSample: 4212---4230   \n",
      "MAPE 21.404707 Conxtloss 40.45152\n",
      "Pred_result: [99.78903  50.796257 47.952694 44.56316  38.55423  37.892776] \n",
      "Real_label [79. 63. 56. 54. 48. 44.]\n",
      "\n",
      "\n",
      "Batch: 236 TestSample: 4248---4266   \n",
      "MAPE 18.838842 Conxtloss 40.45152\n",
      "Pred_result: [715.04126 437.42728 397.14206 338.49863 322.0675  300.80435] \n",
      "Real_label [463. 355. 329. 309. 303. 266.]\n",
      "\n",
      "\n",
      "Batch: 238 TestSample: 4284---4302   \n",
      "MAPE 15.515402 Conxtloss 40.45152\n",
      "Pred_result: [523.4105  499.39194 511.9429  552.9594  605.5095  560.1785 ] \n",
      "Real_label [527. 578. 640. 696. 664. 618.]\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "#初始batch\n",
    "batch_start = 0\n",
    "#结尾batch\n",
    "batch_end = 240\n",
    "#每隔多少取一个\n",
    "batch_interval = 2\n",
    "mape = 0\n",
    "for i in range(batch_start,batch_end,batch_interval):\n",
    "    input1,input2,real_label = get_batch_data(batch_size,i)\n",
    "    #print(input1,input2)\n",
    "    feed_dict={graph['x1']:input1,graph['x2']:input2,graph['Output_Seq']:real_label}\n",
    "    _,loss1,loss2,loss_FC,Pred_Result,Pred_Context_out,context_embed,ADJ_matrix,Output_seq = sess.run([graph['train'],graph['loss1'],graph['loss2'],graph['loss_FC'],graph['Pred_Result'],graph['Pred_Context_out'],graph['context_embed'],graph['ADJ_matrix'],graph['Output_Seq']],feed_dict)\n",
    "    print(\"\\n\")\n",
    "    print(\"Batch:\",i, \"TestSample: {}---{}   \".format(i*18,i*18+18))\n",
    "    print( \"MAPE\",loss2, \"Conxtloss\", lossFC)\n",
    "    print(\"Pred_result:\", Pred_Result[0][:,15],\"\\nReal_label\",Output_seq[0][:,15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(200):\n",
    "    for i in range(batch_number):\n",
    "        input1,input2,real_label = get_batch_data(batch_size,i)\n",
    "        #print(input1,input2)\n",
    "        feed_dict={graph['x1']:input1,graph['x2']:input2,graph['Output_Seq']:real_label}\n",
    "        _,loss1,Pred_Result,loss2,lossFC,ADJ_matrix,Output_seq = sess.run([graph['train'],graph['Pred_Context_out'],graph['Pred_Result'],graph['loss2'],graph['loss_FC'],graph['ADJ_matrix'],graph['Output_Seq']],feed_dict)\n",
    "            \n",
    "        #_,loss1,Pred_Context_out,Pred_Result,Output_Seq,context_embed, loss2,lossFC,ADJ_matrix= sess.run([graph['train'],graph['loss1'],graph['Pred_Context_out'],graph['Pred_Result'],graph['Output_Seq'],graph['context_embed'],graph['loss2'],graph['loss_FC'],graph['ADJ_matrix']],feed_dict)\n",
    "        if(i%240==0):\n",
    "            #print(  \"MSE:\",loss1,\"  MAPE\",loss2,\"预测结果:\",Pred_Result[0],\" 目标: \",real_label[:,0,:])\n",
    "            print( \"Epoch:\", epoch, \"  MAPE\",loss2,\"Conxtloss\", lossFC,\"Pred_result:\", Pred_Result[0][:,15],Output_seq[0][:,15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0   MAPE 35.173004 Conxtloss 12.745356 Pred_result: [109.571045 107.88966  176.85127  256.62006  359.36816  482.67126 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 1   MAPE 35.305824 Conxtloss 12.746304 Pred_result: [113.30425 107.65812 176.00102 254.37044 355.85703 479.72302] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 2   MAPE 36.276558 Conxtloss 12.747824 Pred_result: [117.87327 100.24497 173.27757 253.40921 356.5561  481.73642] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 3   MAPE 34.939743 Conxtloss 12.748443 Pred_result: [111.61251 106.81079 176.02304 255.52875 358.1844  483.37518] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 4   MAPE 34.94538 Conxtloss 12.750341 Pred_result: [112.475136 108.1655   176.36792  255.28423  357.3441   482.09967 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 5   MAPE 34.71719 Conxtloss 12.7500515 Pred_result: [109.83979 106.90159 175.92001 255.51698 358.15787 483.5083 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 6   MAPE 35.087776 Conxtloss 12.748362 Pred_result: [111.04892  106.558014 175.73566  254.80464  356.9117   481.62827 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 7   MAPE 34.957195 Conxtloss 12.749523 Pred_result: [109.64086 105.49533 175.3328  255.05698 357.7595  483.17532] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 8   MAPE 35.975983 Conxtloss 12.743481 Pred_result: [110.4388  104.40711 176.15665 255.30844 357.9035  483.64307] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 9   MAPE 35.35048 Conxtloss 12.751497 Pred_result: [108.907585 103.844696 174.28552  252.64594  354.07877  477.6179  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 10   MAPE 34.99328 Conxtloss 12.750155 Pred_result: [109.52127 105.73949 175.74477 255.0312  357.58286 482.72638] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 11   MAPE 34.793915 Conxtloss 12.748653 Pred_result: [110.17316 107.45556 176.36008 255.20988 357.3047  482.02917] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 12   MAPE 35.916176 Conxtloss 12.751085 Pred_result: [115.622574 100.22512  173.60158  254.01439  357.36212  483.77728 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 13   MAPE 35.696716 Conxtloss 12.754732 Pred_result: [111.40698  97.55189 170.85912 250.96492 353.42847 477.88577] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 14   MAPE 37.544533 Conxtloss 12.759882 Pred_result: [120.485954 102.396645 171.21437  247.6946   345.80005  465.76614 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 15   MAPE 45.25676 Conxtloss 12.754915 Pred_result: [165.27443 104.83487 177.38339 255.23553 356.4156  480.92722] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 16   MAPE 40.346245 Conxtloss 12.755449 Pred_result: [138.49716 100.91916 174.89204 254.1335  355.54877 479.86014] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 17   MAPE 34.89987 Conxtloss 12.754795 Pred_result: [108.17767  98.57163 172.82149 255.15411 359.59644 487.19904] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 18   MAPE 33.487453 Conxtloss 12.754095 Pred_result: [102.3434  106.67679 175.28598 255.52835 357.945   483.7124 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 19   MAPE 33.812458 Conxtloss 12.752257 Pred_result: [105.200005 107.182556 175.40144  254.9773   356.76544  481.9701  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 20   MAPE 34.730118 Conxtloss 12.755073 Pred_result: [109.98592 103.64932 174.08818 254.04854 356.17197 481.39908] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 21   MAPE 34.024826 Conxtloss 12.755102 Pred_result: [104.893105 105.81144  174.74843  254.81433  356.88226  482.29086 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 22   MAPE 33.915188 Conxtloss 12.754992 Pred_result: [100.90442 102.46509 172.42105 252.49551 354.12915 478.9681 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 23   MAPE 34.797924 Conxtloss 12.751827 Pred_result: [109.82452 105.45986 173.89645 252.24821 352.51672 476.1943 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 24   MAPE 44.400818 Conxtloss 12.7520895 Pred_result: [166.1728   116.734924 182.81914  257.02267  355.61502  478.59634 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 25   MAPE 32.830933 Conxtloss 12.751632 Pred_result: [ 94.07716  104.078636 173.226    254.11731  356.43085  482.1319  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 26   MAPE 34.030926 Conxtloss 12.751364 Pred_result: [103.78888 104.82029 173.43292 252.65616 353.5109  477.69366] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 27   MAPE 33.003674 Conxtloss 12.7511425 Pred_result: [ 97.65884 105.92097 174.19023 254.3133  356.01862 481.28848] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 28   MAPE 33.09686 Conxtloss 12.749859 Pred_result: [ 97.06401 104.79433 173.55853 253.8252  355.5655  480.76987] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 29   MAPE 33.11833 Conxtloss 12.750534 Pred_result: [ 97.92482 105.16765 173.61172 253.6463  355.13364 479.5141 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 30   MAPE 34.341652 Conxtloss 12.746985 Pred_result: [109.28883 107.10139 175.92854 255.23877 356.65912 479.27145] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 31   MAPE 34.067093 Conxtloss 12.746983 Pred_result: [106.58043 106.24475 174.54497 253.38062 353.99643 477.83978] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 32   MAPE 34.303215 Conxtloss 12.745691 Pred_result: [109.13702 107.80306 175.72455 254.2382  353.9385  477.75385] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 33   MAPE 33.952457 Conxtloss 12.747348 Pred_result: [106.36131 106.91998 174.82787 253.44519 353.38943 475.8961 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 34   MAPE 34.08577 Conxtloss 12.744876 Pred_result: [108.62186 108.94207 176.92226 256.00314 356.89413 480.4526 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 35   MAPE 33.878178 Conxtloss 12.739253 Pred_result: [105.707466 108.51932  176.52264  256.07245  357.70514  480.70328 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 36   MAPE 34.68946 Conxtloss 12.738059 Pred_result: [108.692924 106.443146 174.46758  252.49132  352.5453   476.731   ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 37   MAPE 34.268307 Conxtloss 12.740152 Pred_result: [108.09895  106.913925 174.13171  251.70914  351.25928  471.85632 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 38   MAPE 33.970497 Conxtloss 12.739745 Pred_result: [105.96952 107.39921 175.30107 253.98071 354.81265 478.9551 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 39   MAPE 34.67991 Conxtloss 12.739824 Pred_result: [111.133705 107.30794  175.91847  254.66635  355.86475  479.73965 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 40   MAPE 34.39947 Conxtloss 12.739155 Pred_result: [109.47057 107.53197 175.57185 253.94264 354.53214 476.59244] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 41   MAPE 34.323208 Conxtloss 12.738418 Pred_result: [109.25695 108.01948 176.3001  255.04314 356.26917 478.77597] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 42   MAPE 34.02855 Conxtloss 12.738197 Pred_result: [107.62957 108.78464 176.65324 255.47885 356.77374 480.04608] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 43   MAPE 34.053387 Conxtloss 12.737783 Pred_result: [107.75829 109.22063 176.54092 254.85507 355.61377 479.43982] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 44   MAPE 33.19799 Conxtloss 12.74285 Pred_result: [ 99.91416 105.25214 172.4418  250.29349 349.06662 470.29678] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 45   MAPE 33.768333 Conxtloss 12.743175 Pred_result: [102.41643  106.323235 174.7128   253.19705  353.67514  475.07553 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 46   MAPE 33.94591 Conxtloss 12.742388 Pred_result: [107.705925 109.83398  177.05933  255.45178  356.2654   478.28528 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 47   MAPE 33.953266 Conxtloss 12.743145 Pred_result: [108.02158 110.22585 177.36661 255.6704  356.4405  478.9652 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 48   MAPE 33.89282 Conxtloss 12.738356 Pred_result: [104.17466 108.51596 177.48427 257.08734 359.26712 483.87955] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 49   MAPE 37.935234 Conxtloss 12.73457 Pred_result: [137.79933 112.32093 181.67622 256.93243 355.8592  477.20618] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 50   MAPE 34.016083 Conxtloss 12.7375965 Pred_result: [102.91296 106.13915 174.16936 251.33136 350.69257 471.95486] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 51   MAPE 33.883404 Conxtloss 12.739646 Pred_result: [103.68715  107.646095 175.50488  253.35493  353.59894  475.09262 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 52   MAPE 33.60872 Conxtloss 12.738692 Pred_result: [103.35806  107.857475 175.31923  253.38239  353.6775   475.16757 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 53   MAPE 33.893536 Conxtloss 12.739397 Pred_result: [107.18772  108.193275 175.93694  254.32762  355.10004  477.18683 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 54   MAPE 34.347084 Conxtloss 12.737259 Pred_result: [104.92509 107.40302 175.7878  253.79585 354.3076  478.71973] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 55   MAPE 33.897865 Conxtloss 12.736614 Pred_result: [106.78332  108.366714 175.11784  252.8359   352.76825  474.20645 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 56   MAPE 34.137024 Conxtloss 12.73596 Pred_result: [108.436966 108.05753  174.99261  252.34631  351.94638  473.24918 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 57   MAPE 34.0158 Conxtloss 12.736247 Pred_result: [107.33481  108.029076 175.66913  253.8316   354.37177  476.70886 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 58   MAPE 33.81859 Conxtloss 12.737207 Pred_result: [105.8542   108.123344 175.72842  253.97119  354.54648  477.46088 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 59   MAPE 33.76084 Conxtloss 12.738316 Pred_result: [105.18689  107.857025 175.5712   253.82498  354.34735  476.718   ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 60   MAPE 33.352886 Conxtloss 12.738939 Pred_result: [102.525085 108.49713  175.0279   252.63516  352.2302   473.7028  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 61   MAPE 33.442993 Conxtloss 12.738373 Pred_result: [104.07571 109.59319 176.11829 253.87645 353.8012  476.17255] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 62   MAPE 33.570858 Conxtloss 12.737502 Pred_result: [104.91111 109.74981 176.25574 253.98666 353.93564 478.41055] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 63   MAPE 33.429737 Conxtloss 12.738909 Pred_result: [103.12356 108.92229 175.47903 253.06566 352.7714  475.12833] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 64   MAPE 33.31761 Conxtloss 12.738392 Pred_result: [102.314384 108.9455   175.78293  253.71602  353.82037  476.7673  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 65   MAPE 35.000668 Conxtloss 12.73752 Pred_result: [113.949745 103.219955 173.38026  251.9835   352.69678  475.084   ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 66   MAPE 35.065372 Conxtloss 12.736244 Pred_result: [115.17935 103.68669 173.20033 251.24963 351.47653 473.25128] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 67   MAPE 34.90925 Conxtloss 12.737346 Pred_result: [113.78949 103.58445 173.95825 252.92032 354.4687  477.8595 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 68   MAPE 34.675507 Conxtloss 12.737085 Pred_result: [111.62171 103.23043 173.46553 252.3391  353.732   476.26495] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 69   MAPE 34.74821 Conxtloss 12.737258 Pred_result: [113.24821 104.0696  173.76668 252.46559 354.22235 476.57483] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 70   MAPE 35.026497 Conxtloss 12.73717 Pred_result: [115.14084 104.44622 174.13525 252.69478 357.45938 479.1478 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 71   MAPE 34.84693 Conxtloss 12.736462 Pred_result: [113.92815 104.13447 174.04408 252.80334 363.27954 478.1495 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 72   MAPE 34.858658 Conxtloss 12.736274 Pred_result: [113.39941  103.381485 173.35057  251.83305  354.42715  475.15176 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 73   MAPE 34.81929 Conxtloss 12.736706 Pred_result: [113.90617 104.09353 173.79303 252.3087  360.59082 475.70972] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 74   MAPE 33.616096 Conxtloss 12.737304 Pred_result: [ 99.633255 107.92109  175.49455  252.86658  374.33368  473.4801  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 75   MAPE 33.4947 Conxtloss 12.73656 Pred_result: [ 99.58314  107.462074 174.92487  252.17526  356.63568  473.14673 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 76   MAPE 33.5942 Conxtloss 12.735878 Pred_result: [ 98.943726 106.93082  175.186    252.97676  376.493    475.67834 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 77   MAPE 33.55404 Conxtloss 12.735643 Pred_result: [ 98.414635 107.06332  175.24078  253.04619  375.37143  476.3282  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 78   MAPE 33.46648 Conxtloss 12.735565 Pred_result: [ 97.639336 106.70906  174.35211  251.55411  374.34476  472.3318  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 79   MAPE 33.721497 Conxtloss 12.735416 Pred_result: [ 98.716576 106.06867  174.34271  251.7189   374.14618  474.2254  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 80   MAPE 33.867836 Conxtloss 12.735632 Pred_result: [ 99.20344 105.11249 173.93314 251.44261 374.44708 473.361  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 81   MAPE 33.80584 Conxtloss 12.737286 Pred_result: [100.1298  105.03974 172.95229 249.64645 372.1307  468.76276] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 82   MAPE 32.697926 Conxtloss 12.739861 Pred_result: [ 90.669716 104.85055  172.1405   249.213    371.13446  467.4853  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 83   MAPE 32.810127 Conxtloss 12.739593 Pred_result: [ 91.873375 105.01206  172.87119  250.40822  372.80667  470.33844 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 84   MAPE 33.27487 Conxtloss 12.741622 Pred_result: [ 95.54708  104.870285 172.31824  249.04237  370.571    467.2408  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 85   MAPE 33.344193 Conxtloss 12.735371 Pred_result: [ 97.277245 105.632385 173.51598  250.44908  371.2917   471.3705  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 86   MAPE 33.255135 Conxtloss 12.734985 Pred_result: [ 96.69833 105.42218 172.6173  249.05571 369.9503  467.06296] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 87   MAPE 34.719666 Conxtloss 12.733309 Pred_result: [108.07542 100.82283 173.06088 252.14423 376.53958 478.78546] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 88   MAPE 34.857674 Conxtloss 12.731541 Pred_result: [114.09499  103.571945 172.95993  250.45753  374.72415  472.38486 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 89   MAPE 34.906326 Conxtloss 12.731302 Pred_result: [114.46123 103.66787 173.0071  250.56786 374.13098 474.45285] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 90   MAPE 34.5335 Conxtloss 12.731831 Pred_result: [111.57642 103.48872 172.53735 250.16562 373.67493 473.90805] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 91   MAPE 34.969208 Conxtloss 12.731991 Pred_result: [115.019264 104.0174   172.9841   250.40732  374.88336  473.1781  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 92   MAPE 34.01973 Conxtloss 12.732061 Pred_result: [102.63743 103.94047 172.61617 249.32196 371.07056 471.55087] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 93   MAPE 33.62429 Conxtloss 12.731922 Pred_result: [100.87242 107.13562 174.91765 251.77    373.68558 475.34344] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 94   MAPE 33.012993 Conxtloss 12.732729 Pred_result: [ 96.737335 106.87271  173.1331   248.95293  369.84714  467.94907 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 95   MAPE 32.758858 Conxtloss 12.734325 Pred_result: [ 92.3603  103.74464 170.60849 246.56183 366.73395 465.05725] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 96   MAPE 33.41121 Conxtloss 12.734154 Pred_result: [ 99.13321  106.744125 173.52075  249.34259  370.40662  469.37912 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 97   MAPE 33.53885 Conxtloss 12.735712 Pred_result: [ 98.97668  105.131546 171.83554  247.07643  367.12286  465.1872  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 98   MAPE 32.102726 Conxtloss 12.73484 Pred_result: [ 88.63173 105.25123 171.26808 247.708   368.51996 467.43896] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 99   MAPE 32.61023 Conxtloss 12.735495 Pred_result: [ 89.91499 102.25658 169.45709 246.06291 366.66788 465.63568] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 100   MAPE 32.43086 Conxtloss 12.730785 Pred_result: [ 89.86502  103.813286 168.24258  242.7971   361.5499   454.20413 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 101   MAPE 32.38782 Conxtloss 12.732355 Pred_result: [ 89.293686 102.75951  167.45776  242.0188   361.30533  455.93042 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 102   MAPE 40.475597 Conxtloss 12.7362 Pred_result: [138.88268 108.23244 172.5323  245.00166 363.9442  456.01462] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 103   MAPE 33.818024 Conxtloss 12.739666 Pred_result: [106.72936 113.73761 175.57106 248.05867 365.8306  459.63724] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 104   MAPE 33.16097 Conxtloss 12.74769 Pred_result: [ 91.14206  98.45119 165.87595 240.90627 359.35397 454.5357 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 105   MAPE 33.128857 Conxtloss 12.748737 Pred_result: [ 92.45713  99.79702 168.63339 245.45406 366.93274 463.12943] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 106   MAPE 32.301743 Conxtloss 12.749546 Pred_result: [ 86.16747   99.742966 168.07259  245.44221  367.29126  463.54205 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 107   MAPE 32.695107 Conxtloss 12.749624 Pred_result: [ 88.09882  98.47828 167.11916 243.9497  365.2944  459.87656] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 108   MAPE 32.730267 Conxtloss 12.7473135 Pred_result: [ 87.93693  98.1322  165.9324  241.82884 361.62976 456.29916] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 109   MAPE 32.752655 Conxtloss 12.747076 Pred_result: [ 88.23101  98.44701 166.92287 243.6061  364.62973 459.38583] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 110   MAPE 32.79852 Conxtloss 12.742949 Pred_result: [ 91.57139 100.19378 166.67284 242.28383 362.23593 455.6443 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 111   MAPE 32.53614 Conxtloss 12.741481 Pred_result: [ 91.92815  101.525856 167.80553  244.08804  364.19464  460.6337  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 112   MAPE 32.329414 Conxtloss 12.741678 Pred_result: [ 87.62778 100.02873 168.32037 245.89049 367.55386 464.59006] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 113   MAPE 32.249813 Conxtloss 12.742023 Pred_result: [ 87.34787 100.08741 167.8347  245.01714 366.6909  462.08945] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 114   MAPE 32.295677 Conxtloss 12.740291 Pred_result: [ 89.468155 101.49019  169.32506  246.77904  368.98267  465.30014 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 115   MAPE 32.943127 Conxtloss 12.73914 Pred_result: [ 96.78876 102.08052 169.61026 246.16443 368.1412  463.71884] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 116   MAPE 32.084476 Conxtloss 12.73917 Pred_result: [ 85.79721  99.6332  167.57993 244.59735 365.87463 461.7753 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 117   MAPE 32.750668 Conxtloss 12.740686 Pred_result: [ 94.975685 102.517456 170.40292  247.37482  369.9926   465.79483 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 118   MAPE 32.22817 Conxtloss 12.740458 Pred_result: [ 87.345535 100.306816 168.32695  245.50327  366.11252  465.1626  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 119   MAPE 31.98767 Conxtloss 12.740955 Pred_result: [ 85.77267 101.10416 169.03859 246.56212 368.6246  464.35757] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 120   MAPE 32.041245 Conxtloss 12.740424 Pred_result: [ 86.60705 101.36561 169.04945 246.19295 368.14322 463.36905] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 121   MAPE 32.0206 Conxtloss 12.740531 Pred_result: [ 87.73041 102.01803 169.717   246.91603 369.23044 464.61652] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 122   MAPE 34.49877 Conxtloss 12.728652 Pred_result: [116.31558 115.76544 181.76317 257.23853 380.6374  477.2706 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 123   MAPE 31.65026 Conxtloss 12.733776 Pred_result: [ 91.12278 106.44108 170.7367  246.2152  367.36478 463.64832] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 124   MAPE 31.720001 Conxtloss 12.732561 Pred_result: [ 92.51671  106.767746 172.82567  249.65501  371.71423  469.7284  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 125   MAPE 31.590422 Conxtloss 12.73171 Pred_result: [ 89.30088  105.431305 172.26726  249.4863   372.03284  469.62076 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 126   MAPE 31.475746 Conxtloss 12.731182 Pred_result: [ 88.19857 105.34461 171.59586 248.33923 370.2972  467.02078] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 127   MAPE 32.113083 Conxtloss 12.733962 Pred_result: [ 92.64995 104.11996 170.62961 247.00862 368.77557 463.79294] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 128   MAPE 39.80934 Conxtloss 12.73321 Pred_result: [138.9601  108.8354  173.45248 244.8169  362.77182 452.80835] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 129   MAPE 37.09021 Conxtloss 12.730213 Pred_result: [126.09988 104.8503  173.86777 248.64514 370.89566 465.74377] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 130   MAPE 36.23249 Conxtloss 12.735956 Pred_result: [116.27115 102.66258 173.01016 249.1817  372.8799  470.95157] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 131   MAPE 36.568047 Conxtloss 12.731368 Pred_result: [119.013985 103.36401  172.5132   247.1019   368.74786  462.56845 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 132   MAPE 38.86559 Conxtloss 12.72959 Pred_result: [133.91705 107.22697 175.09216 249.2259  370.03442 465.45447] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 133   MAPE 36.478226 Conxtloss 12.729198 Pred_result: [119.13757  101.494995 171.55049  246.65076  367.4739   463.05786 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 134   MAPE 38.627346 Conxtloss 12.728521 Pred_result: [133.42816  106.403564 175.0966   250.00241  369.8527   468.4487  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 135   MAPE 44.77183 Conxtloss 12.7312 Pred_result: [166.49553 116.40175 179.40196 252.07584 372.3763  465.22885] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 136   MAPE 36.797302 Conxtloss 12.733271 Pred_result: [120.90199 101.411   172.97997 249.43657 371.92358 469.5301 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 137   MAPE 36.818863 Conxtloss 12.731329 Pred_result: [121.46402 101.50174 172.527   248.3927  370.96487 465.98874] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 138   MAPE 36.622578 Conxtloss 12.731315 Pred_result: [121.0786   102.486244 172.78638  248.40915  369.92136  465.89377 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 139   MAPE 38.182518 Conxtloss 12.729026 Pred_result: [131.58113 105.75227 175.60799 251.38295 373.2191  469.79068] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 140   MAPE 36.08773 Conxtloss 12.7322 Pred_result: [116.95783  100.963806 171.28398  246.96495  369.10635  462.07965 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 141   MAPE 36.475044 Conxtloss 12.730009 Pred_result: [119.203896 101.47232  172.07095  247.97134  370.58435  463.94855 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 142   MAPE 38.498318 Conxtloss 12.727043 Pred_result: [133.21341 105.49523 175.53961 251.18341 373.64328 467.68463] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 143   MAPE 37.96743 Conxtloss 12.727275 Pred_result: [130.28404 105.29888 174.78198 250.20471 372.0907  467.25626] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 144   MAPE 37.943657 Conxtloss 12.726362 Pred_result: [130.83337 106.36026 175.87105 251.62766 372.4055  470.88544] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 145   MAPE 37.87814 Conxtloss 12.7258415 Pred_result: [130.89972 106.90537 176.17203 251.83945 374.13693 467.9435 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 146   MAPE 39.246887 Conxtloss 12.725394 Pred_result: [139.22919 110.68757 178.30077 253.32054 375.383   469.28354] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 147   MAPE 38.324436 Conxtloss 12.725559 Pred_result: [133.66129  107.709206 176.83221  252.32489  374.61188  469.76465 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 148   MAPE 38.398556 Conxtloss 12.725342 Pred_result: [134.1218  108.1029  177.40309 253.23322 375.89175 470.33884] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 149   MAPE 37.55976 Conxtloss 12.724746 Pred_result: [129.15562 107.1445  176.46042 252.6056  375.51947 469.87192] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 150   MAPE 38.878437 Conxtloss 12.724658 Pred_result: [136.69606 108.91273 177.43681 252.674   374.68243 468.21216] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 151   MAPE 37.303223 Conxtloss 12.72308 Pred_result: [127.54165 106.8531  176.1264  252.26208 374.88675 469.30908] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 152   MAPE 37.531868 Conxtloss 12.722243 Pred_result: [129.2629   108.253586 177.14388  253.09218  375.71793  471.6919  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 153   MAPE 37.920647 Conxtloss 12.722094 Pred_result: [131.72733 107.88712 176.87637 252.50931 374.49423 469.61832] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 154   MAPE 31.762014 Conxtloss 12.721763 Pred_result: [ 96.0593  106.07663 173.80792 252.14664 375.2313  471.78787] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 155   MAPE 42.646347 Conxtloss 12.72233 Pred_result: [158.0164  116.65531 179.7251  252.56369 372.42044 463.88498] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 156   MAPE 32.090775 Conxtloss 12.722431 Pred_result: [ 98.29545  106.753876 174.40994  252.41888  375.7169   471.1962  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 157   MAPE 32.593918 Conxtloss 12.720921 Pred_result: [102.900604 109.091545 175.33545  251.83682  373.88553  467.79816 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 158   MAPE 31.234707 Conxtloss 12.721816 Pred_result: [ 91.50759 107.39242 173.9969  251.98141 375.15646 470.2411 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 159   MAPE 31.03336 Conxtloss 12.720493 Pred_result: [ 90.25698 108.23275 173.72968 250.88132 373.04962 466.86075] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 160   MAPE 30.93832 Conxtloss 12.720241 Pred_result: [ 89.54792  108.458885 174.12851  251.52744  374.08618  468.24753 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 161   MAPE 30.70045 Conxtloss 12.720165 Pred_result: [ 87.16993 108.12094 173.6665  251.0457  373.4039  467.42725] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 162   MAPE 31.076687 Conxtloss 12.719925 Pred_result: [ 90.51429 108.31626 174.72841 252.48491 375.8884  470.82547] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 163   MAPE 30.652473 Conxtloss 12.720369 Pred_result: [ 87.32485 108.39839 173.6065  250.63972 372.6824  466.21457] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 164   MAPE 32.2376 Conxtloss 12.720398 Pred_result: [100.18307 107.77022 174.49866 250.91397 373.08325 466.55325] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 165   MAPE 32.38228 Conxtloss 12.721683 Pred_result: [100.35867  108.312195 174.18828  249.67607  370.779    463.12238 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 166   MAPE 32.265366 Conxtloss 12.720362 Pred_result: [ 99.5594  108.00469 174.78261 251.10464 373.02377 466.34308] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 167   MAPE 32.396896 Conxtloss 12.720016 Pred_result: [100.53474 108.14048 174.95616 251.11116 373.0484  466.312  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 168   MAPE 31.765045 Conxtloss 12.719858 Pred_result: [ 96.77247 109.03257 175.53659 252.20119 374.77563 468.7376 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 169   MAPE 40.170242 Conxtloss 12.719928 Pred_result: [145.84454  112.386826 178.80865  251.63591  371.81927  463.81485 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 170   MAPE 31.032797 Conxtloss 12.718178 Pred_result: [ 90.03218 107.62429 173.96986 250.81174 372.90518 466.4213 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 171   MAPE 32.618927 Conxtloss 12.718272 Pred_result: [102.57388 108.57508 175.76442 251.88983 373.96185 467.42514] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 172   MAPE 32.481754 Conxtloss 12.718058 Pred_result: [101.572    108.454414 175.99883  252.52866  375.09766  469.12802 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 173   MAPE 31.074875 Conxtloss 12.718508 Pred_result: [ 91.46978 109.30906 175.656   252.70796 375.31134 469.93384] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 174   MAPE 30.448568 Conxtloss 12.720546 Pred_result: [ 82.78783 105.46843 172.41217 249.78955 372.09357 466.4603 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 175   MAPE 32.71498 Conxtloss 12.719091 Pred_result: [102.76523  107.660355 174.68591  250.0935   371.36798  464.01373 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 176   MAPE 32.26725 Conxtloss 12.71859 Pred_result: [100.42405 108.23373 175.5957  251.86075 374.10706 467.97418] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 177   MAPE 32.11705 Conxtloss 12.718758 Pred_result: [ 99.46707 108.00708 175.43806 251.77124 374.06293 468.0088 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 178   MAPE 32.512024 Conxtloss 12.718203 Pred_result: [101.43106 107.3845  174.93016 250.7701  372.55582 465.8152 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 179   MAPE 32.518444 Conxtloss 12.717548 Pred_result: [101.69075 108.04854 175.16925 250.74225 371.99823 464.97247] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 180   MAPE 33.870724 Conxtloss 12.718502 Pred_result: [110.603615 108.48588  176.43776  251.67422  373.07108  466.1698  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 181   MAPE 43.22839 Conxtloss 12.720648 Pred_result: [161.79402 121.57562 183.21776 254.07341 372.1726  462.0627 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 182   MAPE 41.358444 Conxtloss 12.722228 Pred_result: [151.29393 113.71337 180.87471 254.76068 376.14972 468.94556] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 183   MAPE 38.580906 Conxtloss 12.720819 Pred_result: [136.14174 112.01272 179.04019 252.43529 372.6263  464.1218 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 184   MAPE 38.140507 Conxtloss 12.723506 Pred_result: [131.96529 108.77855 175.6106  247.60359 365.94177 454.92218] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 185   MAPE 40.066498 Conxtloss 12.721928 Pred_result: [144.93724  111.325775 179.50522  253.53299  374.94968  467.49918 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 186   MAPE 30.922735 Conxtloss 12.720775 Pred_result: [ 90.152466 106.94048  175.02783  252.97705  376.78333  472.13757 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 187   MAPE 32.78295 Conxtloss 12.719706 Pred_result: [104.440674 109.17018  175.76938  250.64088  371.2031   463.21628 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 188   MAPE 32.36343 Conxtloss 12.720081 Pred_result: [101.11717 108.4205  175.86372 251.66853 372.91815 467.29654] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 189   MAPE 32.10726 Conxtloss 12.722178 Pred_result: [ 98.24306 106.87119 173.42363 248.28459 367.98135 460.50586] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 190   MAPE 41.14654 Conxtloss 12.7230215 Pred_result: [149.18512  112.610054 179.30711  252.20558  372.01984  463.06793 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 191   MAPE 32.383 Conxtloss 12.721859 Pred_result: [100.793   107.46296 175.4706  251.65187 372.7478  467.95547] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 192   MAPE 38.29478 Conxtloss 12.720509 Pred_result: [135.5448  110.8512  178.87733 251.99004 371.8905  463.31342] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 193   MAPE 32.50143 Conxtloss 12.719982 Pred_result: [101.549286 107.48758  174.78264  249.94423  370.09103  463.59027 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 194   MAPE 32.009834 Conxtloss 12.719057 Pred_result: [ 98.74321 108.09804 175.2407  250.67986 371.06415 465.06732] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 195   MAPE 33.12433 Conxtloss 12.720344 Pred_result: [106.84927 110.52827 177.61983 252.75441 373.68787 467.00064] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 196   MAPE 32.588825 Conxtloss 12.72089 Pred_result: [102.35913 109.15009 176.09747 251.25197 371.5557  464.56677] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 197   MAPE 31.983776 Conxtloss 12.721511 Pred_result: [ 97.8367  106.93112 174.84845 250.8224  371.6279  466.1976 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 198   MAPE 31.656078 Conxtloss 12.720898 Pred_result: [ 95.28164  106.216805 173.92497  249.72583  370.02832  464.5995  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 199   MAPE 32.381203 Conxtloss 12.71983 Pred_result: [100.97435 107.84383 175.70181 251.34863 371.56982 467.41034] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 200   MAPE 32.41595 Conxtloss 12.72014 Pred_result: [101.20061  107.589905 175.72679  251.46214  372.46643  466.85614 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 201   MAPE 32.560112 Conxtloss 12.720351 Pred_result: [102.24985 108.13306 175.49083 250.45665 370.74606 463.37512] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 202   MAPE 37.46677 Conxtloss 12.722887 Pred_result: [128.7243   109.551216 177.23386  249.65492  368.14703  457.93237 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 203   MAPE 37.885784 Conxtloss 12.725323 Pred_result: [130.95564 109.51374 178.34435 252.08481 372.32922 464.01816] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 204   MAPE 35.3649 Conxtloss 12.723776 Pred_result: [115.496155 104.8814   174.88815  249.0648   369.68353  460.7391  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 205   MAPE 38.45166 Conxtloss 12.721941 Pred_result: [134.57286  109.917755 178.11453  250.81006  370.73734  461.1256  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 206   MAPE 34.134907 Conxtloss 12.721539 Pred_result: [108.02369  104.509346 174.91524  250.08165  371.93497  463.98026 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 207   MAPE 36.543232 Conxtloss 12.720297 Pred_result: [123.61068 106.39147 176.2389  249.93954 370.24625 461.09842] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 208   MAPE 37.668133 Conxtloss 12.721344 Pred_result: [130.8732   108.908195 179.08942  253.56166  375.52036  468.3489  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 209   MAPE 36.188652 Conxtloss 12.72069 Pred_result: [121.60617 105.58228 176.6011  250.79614 372.23846 463.80157] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 210   MAPE 35.475685 Conxtloss 12.719713 Pred_result: [117.306786 104.791664 175.81497  250.39255  372.06927  463.8251  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 211   MAPE 34.223816 Conxtloss 12.719649 Pred_result: [109.90064 104.79656 174.69624 249.19789 370.1202  461.22482] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 212   MAPE 33.861767 Conxtloss 12.719333 Pred_result: [107.761734 104.55671  174.1065   248.54085  369.1435   460.04407 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 213   MAPE 34.038074 Conxtloss 12.718535 Pred_result: [109.07209 105.03916 174.64566 249.06915 369.732   460.7941 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 214   MAPE 33.824993 Conxtloss 12.71851 Pred_result: [107.79934 104.76991 174.33496 248.79135 369.4412  460.40326] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 215   MAPE 33.766705 Conxtloss 12.718726 Pred_result: [107.56132 105.05189 174.39404 248.79944 369.37878 460.30295] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 216   MAPE 33.742645 Conxtloss 12.71924 Pred_result: [107.28427  105.025024 174.49092  249.033    369.60464  460.97675 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 217   MAPE 33.67755 Conxtloss 12.719015 Pred_result: [106.98591 105.07846 174.61551 249.28856 370.0035  461.26535] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 218   MAPE 33.556053 Conxtloss 12.718416 Pred_result: [106.49198 105.35314 174.52231 249.01527 369.39487 460.3566 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 219   MAPE 33.627808 Conxtloss 12.718342 Pred_result: [106.79767 105.39224 174.70892 249.22452 369.6267  460.76212] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 220   MAPE 33.518677 Conxtloss 12.71809 Pred_result: [106.10922 105.3904  174.62737 249.1644  369.5706  460.46735] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 221   MAPE 33.483646 Conxtloss 12.718094 Pred_result: [105.80851 105.23776 174.76538 249.55435 370.2819  461.53088] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 222   MAPE 33.36916 Conxtloss 12.71806 Pred_result: [105.0269  105.01415 174.41011 249.08768 369.64963 460.67584] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 223   MAPE 33.40635 Conxtloss 12.717915 Pred_result: [105.46137 105.26383 174.74045 249.47423 369.88275 461.60092] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 224   MAPE 33.304398 Conxtloss 12.717452 Pred_result: [104.74541 105.04098 174.46709 249.15535 369.29572 461.23438] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 225   MAPE 33.262993 Conxtloss 12.717547 Pred_result: [104.63307 105.21278 174.68132 249.49736 369.62823 462.1178 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 226   MAPE 33.199936 Conxtloss 12.717489 Pred_result: [104.0972  104.84043 174.28946 248.99756 368.99918 461.26193] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 227   MAPE 33.14309 Conxtloss 12.717823 Pred_result: [103.978935 105.23377  174.45732  249.17537  368.851    461.84082 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 228   MAPE 33.165386 Conxtloss 12.717482 Pred_result: [103.72868 104.71502 173.95604 248.52911 368.36435 459.99   ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 229   MAPE 33.077347 Conxtloss 12.717524 Pred_result: [103.41612  104.919106 174.28928  249.12402  368.9446   461.8912  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 230   MAPE 32.95131 Conxtloss 12.71748 Pred_result: [102.68022 104.82267 174.21626 249.14272 368.97128 461.9226 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 231   MAPE 32.96409 Conxtloss 12.717403 Pred_result: [102.9568  104.94795 174.48816 249.48415 369.60144 462.25064] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 232   MAPE 35.596085 Conxtloss 12.7173605 Pred_result: [119.27394 106.21204 176.1692  250.44518 369.3122  463.18942] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 233   MAPE 36.798386 Conxtloss 12.718329 Pred_result: [125.885574 107.02472  177.81848  251.3764   371.83395  463.30145 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 234   MAPE 34.447243 Conxtloss 12.719764 Pred_result: [112.30712 105.64118 176.25099 251.26308 372.38705 465.3157 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 235   MAPE 34.214294 Conxtloss 12.718456 Pred_result: [111.451065 106.00866  176.1159   250.87482  371.48618  464.36896 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 236   MAPE 32.41574 Conxtloss 12.718855 Pred_result: [100.204605 105.384056 174.5681   250.09206  370.44116  463.89548 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 237   MAPE 33.9943 Conxtloss 12.718092 Pred_result: [110.054184 105.57595  175.3386   249.79033  369.7065   462.2961  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 238   MAPE 34.35416 Conxtloss 12.717985 Pred_result: [112.26007 105.85927 175.57755 249.65274 369.43842 461.2999 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 239   MAPE 34.190517 Conxtloss 12.717501 Pred_result: [111.553955 106.11134  176.15184  250.71542  371.0819   463.74957 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 240   MAPE 34.131435 Conxtloss 12.717239 Pred_result: [111.28144 106.20042 176.21867 250.87148 371.29688 464.149  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 241   MAPE 38.311615 Conxtloss 12.716915 Pred_result: [136.2597   110.724976 179.91512  252.57939  371.64575  464.43787 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 242   MAPE 34.127316 Conxtloss 12.718246 Pred_result: [110.53277 105.20841 176.15106 251.41486 372.10336 467.05695] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 243   MAPE 33.94343 Conxtloss 12.717255 Pred_result: [109.98101  105.628586 176.07738  251.06602  371.73126  465.07526 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 244   MAPE 33.990913 Conxtloss 12.717158 Pred_result: [110.37904 105.61932 175.85016 250.51007 370.5588  464.01367] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 245   MAPE 33.56095 Conxtloss 12.716933 Pred_result: [107.88121 105.66027 176.12274 251.47382 372.0575  466.73462] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 246   MAPE 34.01895 Conxtloss 12.716918 Pred_result: [110.181915 105.03444  175.77809  250.56989  370.54144  464.5119  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 247   MAPE 33.998386 Conxtloss 12.716541 Pred_result: [109.797356 104.63101  175.4606   250.24968  370.0207   464.87805 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 248   MAPE 33.75423 Conxtloss 12.7167 Pred_result: [108.34893 104.75856 175.5695  250.6362  371.3823  464.7248 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 249   MAPE 33.774952 Conxtloss 12.71672 Pred_result: [109.40408 105.64021 175.84009 250.48695 370.3818  464.4979 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 250   MAPE 33.54705 Conxtloss 12.716472 Pred_result: [107.969795 105.55426  175.89531  250.8727   371.0649   465.30658 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 251   MAPE 33.427887 Conxtloss 12.716614 Pred_result: [106.537926 104.590256 175.23488  250.26349  370.20697  464.77948 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 252   MAPE 33.623615 Conxtloss 12.716446 Pred_result: [107.94034 104.85263 175.16864 249.67903 369.34857 462.97714] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 253   MAPE 33.711113 Conxtloss 12.716234 Pred_result: [108.6392  105.08039 175.38622 249.80089 369.5223  462.93744] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 254   MAPE 33.302734 Conxtloss 12.716272 Pred_result: [106.08777  104.977264 175.386    250.34557  370.026    465.17233 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 255   MAPE 33.493855 Conxtloss 12.716247 Pred_result: [107.09195 104.85081 175.2911  249.97743 369.94244 463.35843] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 256   MAPE 33.61219 Conxtloss 12.71585 Pred_result: [108.53478 105.77184 175.66891 250.00653 369.14426 463.57126] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 257   MAPE 33.436028 Conxtloss 12.715908 Pred_result: [107.6408  105.89852 176.183   251.07748 371.4001  465.0345 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 258   MAPE 33.53181 Conxtloss 12.715983 Pred_result: [108.339195 106.14807  176.31488  250.9615   370.3841   465.9233  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 259   MAPE 32.95459 Conxtloss 12.715752 Pred_result: [104.95882 105.9791  176.03957 251.12383 371.2465  465.96808] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 260   MAPE 33.43859 Conxtloss 12.715548 Pred_result: [107.8835  106.53413 177.46881 252.80437 374.2465  468.56213] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 261   MAPE 33.103653 Conxtloss 12.7154 Pred_result: [104.73165 105.1668  176.25012 251.51112 372.52222 467.1216 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 262   MAPE 33.45833 Conxtloss 12.715738 Pred_result: [107.68357 106.23704 176.71585 251.42728 372.5223  465.56586] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 263   MAPE 33.060085 Conxtloss 12.715658 Pred_result: [104.44106 105.21261 176.25684 251.54523 373.10437 466.63748] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 264   MAPE 33.38412 Conxtloss 12.715747 Pred_result: [107.41558 106.28968 176.46689 251.00871 371.74005 464.3944 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 265   MAPE 33.387104 Conxtloss 12.715839 Pred_result: [107.30167 106.19092 176.60152 251.33571 372.09915 465.78195] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 266   MAPE 33.217033 Conxtloss 12.715473 Pred_result: [105.77674 105.62288 176.64056 251.90785 372.99548 468.12198] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 267   MAPE 33.118073 Conxtloss 12.71545 Pred_result: [105.90376 106.25143 176.30295 250.96579 371.02332 465.91235] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 268   MAPE 33.04866 Conxtloss 12.715466 Pred_result: [105.53789 106.33326 176.57086 251.57913 372.60675 466.34885] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 269   MAPE 33.175262 Conxtloss 12.715362 Pred_result: [106.449936 106.34924  176.54065  251.2562   371.2217   466.86478 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 270   MAPE 33.15479 Conxtloss 12.71558 Pred_result: [105.583305 105.664024 176.51773  251.55669  372.40463  467.15012 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 271   MAPE 33.57418 Conxtloss 12.715563 Pred_result: [108.68438  106.253716 176.54092  250.76877  370.47327  466.18427 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 272   MAPE 33.34676 Conxtloss 12.715259 Pred_result: [107.67749  106.667755 176.85106  251.42856  371.6296   466.756   ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 273   MAPE 33.267975 Conxtloss 12.715122 Pred_result: [106.42368 105.70579 176.39159 251.04857 371.77982 465.95132] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 274   MAPE 33.021805 Conxtloss 12.71503 Pred_result: [105.76904  106.515396 176.79324  251.69847  372.03647  468.2326  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 275   MAPE 32.891098 Conxtloss 12.714727 Pred_result: [104.40883 105.63837 176.02856 250.70494 371.29697 465.46454] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 276   MAPE 33.213974 Conxtloss 12.714703 Pred_result: [106.778076 106.23332  176.28627  250.49673  370.18552  465.4092  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 277   MAPE 33.11983 Conxtloss 12.714512 Pred_result: [106.30982 106.39623 176.67006 251.28674 371.41    467.09216] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 278   MAPE 32.996086 Conxtloss 12.714141 Pred_result: [106.254845 106.920204 176.83263  251.39618  372.10922  465.51553 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 279   MAPE 33.147892 Conxtloss 12.714346 Pred_result: [107.165016 106.955795 176.89651  251.29358  371.68982  465.8271  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 280   MAPE 33.16992 Conxtloss 12.714161 Pred_result: [106.89863  106.607285 176.98767  251.65216  371.88162  467.68634 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 281   MAPE 33.03157 Conxtloss 12.7139845 Pred_result: [105.55352  105.802925 176.41049  251.04869  372.0429   465.3239  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 282   MAPE 32.446846 Conxtloss 12.714096 Pred_result: [103.165306 106.88729  176.093    250.33835  369.70715  465.13025 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 283   MAPE 33.041 Conxtloss 12.713987 Pred_result: [105.58485 105.47945 175.98941 250.25294 370.40607 464.80682] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 284   MAPE 32.91718 Conxtloss 12.713726 Pred_result: [106.004196 106.956665 176.89723  251.39221  371.09534  467.62888 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 285   MAPE 36.39681 Conxtloss 12.71361 Pred_result: [125.93285 108.12611 179.04652 251.57907 371.2801  463.9532 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 286   MAPE 32.984974 Conxtloss 12.715819 Pred_result: [104.898415 105.15394  176.3836   251.41492  372.06146  468.25473 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 287   MAPE 32.559685 Conxtloss 12.714626 Pred_result: [102.76212 105.26582 175.82509 250.59659 370.9333  466.3677 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 288   MAPE 32.62108 Conxtloss 12.714103 Pred_result: [104.012   106.143   176.17303 250.63306 370.7533  465.37155] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 289   MAPE 32.747063 Conxtloss 12.7140045 Pred_result: [104.18083 105.46295 175.72794 249.93492 370.38007 463.1106 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 290   MAPE 32.447517 Conxtloss 12.713861 Pred_result: [102.50205 105.44834 175.64484 250.12352 370.75168 463.81393] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 291   MAPE 32.747505 Conxtloss 12.713753 Pred_result: [105.51972 106.72476 176.35588 250.46298 370.2476  464.67606] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 292   MAPE 31.923946 Conxtloss 12.714011 Pred_result: [100.412155 106.47186  175.6696   250.15248  370.23056  463.9451  ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 293   MAPE 32.095863 Conxtloss 12.713795 Pred_result: [101.896904 106.907715 176.00665  250.26776  370.16675  464.04034 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 294   MAPE 32.63439 Conxtloss 12.713267 Pred_result: [103.63731 105.60535 175.92819 250.22462 369.8982  465.32275] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 295   MAPE 32.336098 Conxtloss 12.713421 Pred_result: [101.98478 105.44714 175.8144  250.4221  370.92435 465.2135 ] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 296   MAPE 32.568497 Conxtloss 12.713388 Pred_result: [103.3805  105.33065 175.64459 249.85773 369.6852  464.54987] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 297   MAPE 32.30962 Conxtloss 12.713385 Pred_result: [101.84749 105.34391 175.64932 250.12271 370.138   464.99213] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 298   MAPE 32.50223 Conxtloss 12.713267 Pred_result: [103.12058 105.38757 175.7007  249.96364 370.2018  463.64185] [ 75. 102. 180. 300. 396. 525.]\n",
      "Epoch: 299   MAPE 32.406418 Conxtloss 12.713375 Pred_result: [102.49231 105.24753 175.56047 249.8404  369.94598 463.7733 ] [ 75. 102. 180. 300. 396. 525.]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(300):\n",
    "    for i in range(batch_number):\n",
    "        input1,input2,real_label = get_batch_data(batch_size,i)\n",
    "        #print(input1,input2)\n",
    "        feed_dict={graph['x1']:input1,graph['x2']:input2,graph['Output_Seq']:real_label}\n",
    "        _,loss1,Pred_Result,loss2,lossFC,ADJ_matrix,Output_seq = sess.run([graph['train'],graph['Pred_Context_out'],graph['Pred_Result'],graph['loss2'],graph['loss_FC'],graph['ADJ_matrix'],graph['Output_Seq']],feed_dict)\n",
    "            \n",
    "        #_,loss1,Pred_Context_out,Pred_Result,Output_Seq,context_embed, loss2,lossFC,ADJ_matrix= sess.run([graph['train'],graph['loss1'],graph['Pred_Context_out'],graph['Pred_Result'],graph['Output_Seq'],graph['context_embed'],graph['loss2'],graph['loss_FC'],graph['ADJ_matrix']],feed_dict)\n",
    "        if(i%240==0):\n",
    "            #print(  \"MSE:\",loss1,\"  MAPE\",loss2,\"预测结果:\",Pred_Result[0],\" 目标: \",real_label[:,0,:])\n",
    "            print( \"Epoch:\", epoch, \"  MAPE\",loss2,\"Conxtloss\", lossFC,\"Pred_result:\", Pred_Result[0][:,15],Output_seq[0][:,15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 7086.675   MAPE 31.098145  FC_loss_mape: 12.713832\n",
      "预测结果: [ 93.763824 140.72855  155.22928   83.48794   97.77398 ] \n",
      " 目标:  [ 75.  79. 172.  74. 146.]\n",
      "MSE: 42542.367   MAPE 39.518364  FC_loss_mape: 14.220515\n",
      "预测结果: [137.90341 198.57657 211.73291 117.81196 137.03917] \n",
      " 目标:  [143. 117. 274. 118. 267.]\n",
      "MSE: 32347.195   MAPE 36.78245  FC_loss_mape: 13.939212\n",
      "预测结果: [133.1827  193.87022 195.4505  102.19683 128.1573 ] \n",
      " 目标:  [144. 107. 268. 113. 241.]\n",
      "MSE: 2113.334   MAPE 48.84018  FC_loss_mape: 10.0731\n",
      "预测结果: [ 95.75968  118.834785 154.40482  103.11043  152.98051 ] \n",
      " 目标:  [ 48.  49. 119.  47.  73.]\n",
      "MSE: 76461.06   MAPE 30.125385  FC_loss_mape: 14.14144\n",
      "预测结果: [ 92.68455 150.70296 150.1455   81.70289  97.44717] \n",
      " 目标:  [128. 113. 117.  49.  72.]\n",
      "MSE: 38661.773   MAPE 38.105804  FC_loss_mape: 14.190813\n",
      "预测结果: [ 90.0662   146.09795  147.25763   78.119415  93.577576] \n",
      " 目标:  [187. 151. 143.  68. 122.]\n",
      "MSE: 37375.48   MAPE 30.20583  FC_loss_mape: 14.558494\n",
      "预测结果: [117.10807  173.80698  163.77063   81.998245 104.616974] \n",
      " 目标:  [183. 161. 128.  71. 120.]\n",
      "MSE: 10842.882   MAPE 26.127502  FC_loss_mape: 13.268246\n",
      "预测结果: [ 93.04211 138.76534 136.36711  70.42946  85.70323] \n",
      " 目标:  [ 86.  86. 158.  99. 118.]\n",
      "MSE: 92801.21   MAPE 30.726654  FC_loss_mape: 14.865587\n",
      "预测结果: [119.638626 177.3021   169.01167   85.63245  106.97478 ] \n",
      " 目标:  [132. 112. 253. 160. 182.]\n",
      "MSE: 112322.15   MAPE 31.20552  FC_loss_mape: 14.850727\n",
      "预测结果: [141.36914 208.33192 200.73198  98.62766 131.82422] \n",
      " 目标:  [119. 117. 251. 187. 190.]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Context_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-4cd6fd61874e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/GNN_output.npy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_data2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/GNN_realdata.npy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/GNN_context_matrix.npy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mContext_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Context_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "#保存最后一次训练的数据，可以用作评估(不要改动batch_size),都存为240*108*6\n",
    "#存取FC的预测 \n",
    "output_data1 = np.ones((one_epoch_size*batch_size,108,6))\n",
    "#存取GNN的预测\n",
    "output_data2 = np.ones((one_epoch_size*batch_size,108,6))\n",
    "#存取对应的真实值\n",
    "real_data = np.ones((one_epoch_size*batch_size,108,6))\n",
    "\n",
    "for i in range(batch_number):\n",
    "        input1,input2,real_label = get_batch_data(batch_size,i)\n",
    "        \n",
    "        #训练并得到相应的预测和loss\n",
    "        feed_dict={graph['x1']:input1,graph['x2']:input2,graph['Output_Seq']:real_label}\n",
    "        _,loss1,loss2,loss_FC,Pred_Result,Pred_Context_out= sess.run([graph['train'],graph['loss1'],graph['loss2'],graph['loss_FC'],graph['Pred_Result'],graph['Pred_Context_out']],feed_dict)\n",
    "        \n",
    "        #将每一个batch的数据存起来\n",
    "        output_data1[i,:,:] = Pred_Context_out.transpose(0,2,1)[0]\n",
    "        real_data[i] = real_label.transpose(0,2,1)[0]\n",
    "        for j in range(6):\n",
    "            output_data2[i,:,j] = Pred_Result[0][j,:]\n",
    "        \n",
    "        if(i%24==0):\n",
    "            #print(  \"MSE:\",loss1,\"  MAPE\",loss2,\"预测结果:\",Pred_Result[0],\" 目标: \",real_label[:,0,:])\n",
    "            print(  \"MSE:\",loss1,\"  MAPE\",loss2, \" FC_loss_mape:\",loss_FC)\n",
    "            print(\"预测结果:\",Pred_Result[0][0,15:20],\"\\n 目标: \",real_label[0,0,15:20])\n",
    "np.save('data/Context_output.npy',output_data1)\n",
    "np.save('data/GNN_output.npy',output_data2)\n",
    "np.save('data/GNN_realdata.npy',real_data)\n",
    "#np.save('data/GNN_context_matrix.npy',Context_matrix.squeeze(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4303, 108, 6), (1, 6, 108))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data2.shape, Pred_Result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4303"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_epoch_size*batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "SIP_E = np.load('ex_time_stamp_30min.npy')\n",
    "SIP_30min_flow = np.load('Taxipick_sum_30m.npy')\n",
    "SIP_sum = np.int32(np.sum(SIP_30min_flow, axis = 1))\n",
    "SIP_sum_new = np.where(SIP_sum>1000, 1,0)\n",
    "index = np.flatnonzero(SIP_sum_new)\n",
    "SIP_30min_flow_new = np.zeros([index.shape[0],SIP_30min_flow.shape[1]])\n",
    "for i, c in enumerate(list(index)):\n",
    "    SIP_30min_flow_new[i,:]=SIP_30min_flow[c,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adj_NYC = np.load('static_affinity.npy')\n",
    "Adj_NYC.shape\n",
    "new_adj = np.zeros([70,354])\n",
    "new_adj_f = np.zeros([70,70]) \n",
    "for i, c in enumerate(list(index)):\n",
    "    new_adj[i,:] = Adj_NYC[c,:]\n",
    "for j, c in enumerate(list(index)):\n",
    "    new_adj_f[:,j] = new_adj[:,c] \n",
    "SIP_30min_flow_new1 = np.zeros([70, 7248])\n",
    "SIP_30min_flow_new1 = np.log(SIP_30min_flow_new+1)\n",
    "np.save('NYC70_flow_30min_log.npy',SIP_30min_flow_new1)\n",
    "np.save('NYC70_flow_30min.npy',SIP_30min_flow_new)\n",
    "np.save('NYC70_Adj.npy',new_adj_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.73396765, 0.        , 0.75147227, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.84960787, 0.        , 0.        , 0.        , 0.6620576 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.86324318, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.72947562, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.67672079, 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_adj_f[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SIP_30min_flow_new[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-150-8997ea920a91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mSIP_30min_flow_new1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSIP_30min_flow_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.38629436, 0.69314718, 2.30258509, ..., 0.69314718, 0.69314718,\n",
       "        0.        ],\n",
       "       [1.60943791, 2.30258509, 2.63905733, ..., 0.69314718, 0.        ,\n",
       "        0.69314718],\n",
       "       [1.94591015, 2.89037176, 3.4657359 , ..., 1.09861229, 0.69314718,\n",
       "        1.09861229],\n",
       "       ...,\n",
       "       [2.07944154, 2.19722458, 2.07944154, ..., 1.09861229, 1.09861229,\n",
       "        1.09861229],\n",
       "       [5.19295685, 4.44265126, 4.47733681, ..., 5.35185813, 5.29330482,\n",
       "        5.40267738],\n",
       "       [0.69314718, 1.38629436, 1.38629436, ..., 0.        , 1.09861229,\n",
       "        0.69314718]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SIP_30min_flow_new1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
